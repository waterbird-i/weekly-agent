"""
Weekly ç”Ÿæˆå™¨ä¸»æ¨¡å—
è´Ÿè´£åè°ƒå„æ¨¡å—ç”Ÿæˆå‰ç«¯ Weekly
"""

import json
import logging
import os
import re
from datetime import datetime, timezone
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
from urllib.parse import urljoin, urlparse

import yaml
from openai import OpenAI

from ..core.rss_fetcher import RSSFetcher, Article
from ..core.content_filter import ContentFilter
from ..fetchers.leetcode_fetcher import LeetCodeFetcher
from ..fetchers.web_fetcher import WebFetcher
from ..formatters.weekly_formatter import WeeklyFormatter, WeeklyItem
from ..utils import truncate_text, URLDeduplicator, create_retry_session

logger = logging.getLogger(__name__)


class WeeklyGenerator:
    """å‰ç«¯ Weekly ç”Ÿæˆå™¨"""
    
    def __init__(self, config_path: str = "weekly_config.yaml"):
        """
        åˆå§‹åŒ–ç”Ÿæˆå™¨
        
        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        """
        self.config_path = Path(config_path)
        # é¡¹ç›®æ ¹ç›®å½• (src/generators -> src -> project_root)
        self.project_root = Path(__file__).parent.parent.parent
        if not self.config_path.is_absolute():
            self.config_path = self.project_root / config_path
        
        self.config = self._load_config()
        self.state_file = self._get_state_file()
        self.deduplicator = self._init_deduplicator()
        self.http_session = create_retry_session(total_retries=2, backoff_factor=0.8)
        self._page_image_cache: Dict[str, str] = {}
        self._init_ai_client()
    
    def _load_config(self) -> Dict[str, Any]:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            logger.info(f"é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ: {self.config_path}")
            return config
        except Exception as e:
            logger.error(f"åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
            raise
    
    def _get_state_file(self) -> Path:
        """è·å– issue çŠ¶æ€æ–‡ä»¶è·¯å¾„"""
        state_file = self.config.get('state', {}).get('issue_file', 'cache/weekly_state.json')
        path = Path(state_file)
        if not path.is_absolute():
            path = self.project_root / path
        path.parent.mkdir(parents=True, exist_ok=True)
        return path

    def _load_state(self) -> Dict[str, Any]:
        """åŠ è½½è¿è¡ŒçŠ¶æ€"""
        if not self.state_file.exists():
            return {}
        try:
            with open(self.state_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            if isinstance(data, dict):
                return data
        except Exception as e:
            logger.warning(f"è¯»å–çŠ¶æ€æ–‡ä»¶å¤±è´¥ï¼Œå°†å›é€€é»˜è®¤é…ç½®: {self.state_file}, é”™è¯¯: {e}")
        return {}

    def _save_state(self, state: Dict[str, Any]):
        """ä¿å­˜è¿è¡ŒçŠ¶æ€"""
        try:
            self.state_file.parent.mkdir(parents=True, exist_ok=True)
            with open(self.state_file, 'w', encoding='utf-8') as f:
                json.dump(state, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.error(f"ä¿å­˜çŠ¶æ€æ–‡ä»¶å¤±è´¥: {self.state_file}, é”™è¯¯: {e}")

    def _init_deduplicator(self) -> Optional[URLDeduplicator]:
        """åˆå§‹åŒ– Weekly å»é‡å™¨"""
        dedup_cfg = self.config.get('dedup', {})
        cache_file = dedup_cfg.get('cache_file')
        if not cache_file:
            return None
        cache_path = Path(cache_file)
        if not cache_path.is_absolute():
            cache_path = self.project_root / cache_path
        expire_hours = dedup_cfg.get('cache_expire_hours', 720)
        return URLDeduplicator(str(cache_path), expire_hours)
    
    def _init_ai_client(self):
        """åˆå§‹åŒ– AI å®¢æˆ·ç«¯"""
        ai_config = self.config.get('ai', {})
        api_key_env = ai_config.get('api_key_env', 'AI_API_KEY')
        api_key = os.getenv(api_key_env) or ai_config.get('api_key', '')
        if not api_key:
            logger.warning(f"æœªæ£€æµ‹åˆ° Weekly AI API Keyï¼Œè¯·è®¾ç½®ç¯å¢ƒå˜é‡ {api_key_env}")

        self.ai_client = OpenAI(
            api_key=api_key,
            base_url=ai_config.get('api_base', 'https://200.xstx.info/v1')
        )
        self.ai_model = ai_config.get('model', 'claude-opus-4-5-20251101-thinking')
        self.ai_max_tokens = ai_config.get('max_tokens', 4096)
        self.weekly_prompt = ai_config.get('weekly_prompt', '')
    

    
    def get_current_issue(self) -> int:
        """è·å–å½“å‰æœŸå·"""
        state = self._load_state()
        if isinstance(state.get('current_issue'), int):
            return state['current_issue']
        return self.config.get('weekly', {}).get('current_issue', 1)

    def _set_next_issue(self, issue: int):
        """æ›´æ–°ä¸‹ä¸€æœŸå·åˆ°çŠ¶æ€æ–‡ä»¶"""
        state = self._load_state()
        state['current_issue'] = issue + 1
        state['updated_at'] = datetime.now().isoformat()
        self._save_state(state)
    
    def get_current_date(self) -> str:
        """è·å–å½“å‰æ—¥æœŸå­—ç¬¦ä¸²"""
        date_format = self.config.get('weekly', {}).get('date_format', '%Y%m%d')
        return datetime.now().strftime(date_format)
    
    def get_output_path(self, issue: int, date: str) -> str:
        """è·å–è¾“å‡ºæ–‡ä»¶è·¯å¾„"""
        template = self.config.get('weekly', {}).get(
            'output_template', 
            'output/NO{issue}.å‰ç«¯Weekly({date}).md'
        )
        path = template.format(issue=issue, date=date)
        if not Path(path).is_absolute():
            path = str(self.project_root / path)
        return path
    
    def _fetch_category_articles(
        self, 
        category_config: Dict[str, Any]
    ) -> List[Article]:
        """
        è·å–æŸä¸ªåˆ†ç±»çš„æ–‡ç« 
        
        Args:
            category_config: åˆ†ç±»é…ç½®
            
        Returns:
            æ–‡ç« åˆ—è¡¨
        """
        feeds = category_config.get('feeds', [])
        if not feeds:
            return []
        
        # åˆ†ç¦» RSS å’Œ æ™®é€šç½‘é¡µ
        rss_feeds = []
        web_urls = []
        
        for feed in feeds:
            url = feed.get('url', '')
            # ç®€å•åˆ¤æ–­æ˜¯å¦ä¸º RSSï¼Œå¾®ä¿¡å…¬ä¼—å·æ–‡ç« é“¾æ¥è‚¯å®šä¸æ˜¯ RSS
            if 'mp.weixin.qq.com' in url or not (url.endswith('.xml') or url.endswith('.rss') or url.endswith('.atom') or 'rss' in url.lower() or 'feed' in url.lower()):
                web_urls.append(feed)
            else:
                rss_feeds.append(feed)
        
        articles = []
        
        # 1. æŠ“å– RSS
        if rss_feeds:
            fetcher = RSSFetcher(rss_feeds)
            articles.extend(fetcher.fetch_all())
        
        # 2. æŠ“å–æ™®é€šç½‘é¡µ
        if web_urls:
            web_fetcher = WebFetcher()
            articles.extend(web_fetcher.fetch_all(web_urls))
        
        if not articles:
            return []
        
        # æ—¶é—´å’Œé•¿åº¦è¿‡æ»¤é…ç½®
        time_hours = self.config.get('time_filter', {}).get('hours', 168)
        pre_filter_config = self.config.get('pre_filter', {})
        min_length = pre_filter_config.get('min_content_length', 50)
        
        filter_config = {
            'time_filter': {'hours': time_hours},
            'pre_filter': {
                'include_keywords': category_config.get('keywords', []),
                'exclude_keywords': [],
                'min_content_length': min_length
            }
        }
        
        content_filter = ContentFilter(filter_config)
        filtered = content_filter.apply_all_filters(articles)
        
        return filtered

    def _extract_candidate_links(self, article: Article) -> List[Tuple[str, str]]:
        """
        ä»èšåˆå†…å®¹ä¸­æå–å€™é€‰é“¾æ¥ï¼ˆç”¨äºæ¡ç›®çº§é“¾æ¥åˆ†é…ï¼‰
        """
        text = f"{article.content or ''}\n{article.summary or ''}"
        candidates: List[Tuple[str, str]] = []
        seen_urls = set()

        # markdown é“¾æ¥: [title](url)
        for match in re.finditer(r'\[([^\]]{2,200})\]\((https?://[^\s)]+)\)', text):
            anchor = match.group(1).strip()
            url = match.group(2).strip().rstrip(').,;')
            if not url or url == article.url or url in seen_urls:
                continue
            if self._is_noise_source_link(anchor, url):
                continue
            seen_urls.add(url)
            candidates.append((anchor, url))

        # è£¸é“¾æ¥: https://...
        for match in re.finditer(r'(https?://[^\s<>()]+)', text):
            url = match.group(1).strip().rstrip(').,;')
            if not url or url == article.url or url in seen_urls:
                continue
            if self._is_noise_source_link("", url):
                continue
            seen_urls.add(url)
            candidates.append(("", url))

        # å¦‚æœæ­£æ–‡é‡Œé“¾æ¥å¾ˆå°‘ï¼Œå°è¯•ä»æ¥æºç½‘é¡µæŠ½å–æ›´å¤šå€™é€‰é“¾æ¥
        if len(candidates) <= 1 and article.url.startswith("http"):
            for anchor, url in self._extract_links_from_source_page(article.url):
                if url == article.url or url in seen_urls:
                    continue
                seen_urls.add(url)
                candidates.append((anchor, url))

        return candidates

    def _extract_links_from_source_page(self, source_url: str) -> List[Tuple[str, str]]:
        """
        ä»æ¥æºé¡µé¢ä¸­æå–æ­£æ–‡é”šç‚¹é“¾æ¥ï¼Œè¡¥å……æ¡ç›®çº§ URL å€™é€‰
        """
        try:
            response = self.http_session.get(source_url, timeout=15)
            response.raise_for_status()
            html = response.text
            from bs4 import BeautifulSoup
            soup = BeautifulSoup(html, 'html.parser')

            root = None
            for selector in ['main', 'article', '.content', '#content', '.post-content', '.rich_media_content', 'body']:
                node = soup.select_one(selector)
                if node:
                    root = node
                    break

            if not root:
                return []

            result: List[Tuple[str, str]] = []
            seen = set()
            for anchor in root.select('a[href]'):
                href = (anchor.get('href') or '').strip()
                text = anchor.get_text(" ", strip=True)
                if not href:
                    continue
                if href.startswith('#') or href.startswith('javascript:'):
                    continue

                full_url = urljoin(source_url, href).strip()
                if not full_url.startswith('http'):
                    continue
                if full_url in seen:
                    continue
                if self._is_noise_source_link(text, full_url):
                    continue

                seen.add(full_url)
                result.append((text, full_url))
            return result
        except Exception as e:
            logger.debug(f"æ¥æºé¡µé“¾æ¥è¡¥å……å¤±è´¥: {source_url}, é”™è¯¯: {e}")
            return []

    def _is_noise_source_link(self, text: str, url: str) -> bool:
        """è¿‡æ»¤æ¥æºé¡µä¸­çš„å¯¼èˆª/ç¤¾äº¤/ç´ æé“¾æ¥"""
        lower_text = (text or "").lower()
        lower_url = url.lower()

        text_noise = (
            "å…³äºæˆ‘",
            "åŒæ€§äº¤å‹",
            "è¿›ç¾¤",
            "è®¿é—®ç½‘é¡µç‰ˆ",
            "å°é…’é¦†",
            "è‡ªåª’ä½“",
            "å‰å¾€å®˜ç½‘æŸ¥çœ‹å®Œæ•´ç‰ˆ",
            "é˜…è¯»å…¨æ–‡",
            "ç‚¹å‡»æŸ¥çœ‹åŸæ–‡",
            "åŸæ–‡é“¾æ¥",
        )
        if any(keyword.lower() in lower_text for keyword in text_noise):
            return True

        url_noise = ("logo", "avatar", "favicon", ".jpg", ".jpeg", ".png", ".gif", ".svg")
        if any(keyword in lower_url for keyword in url_noise):
            return True

        parsed = urlparse(lower_url)
        if parsed.netloc == "ai.hubtoday.app" and parsed.path.strip("/") == "":
            return True

        if "github.com/justlovemaki" in lower_url:
            return True

        return False

    def _build_link_candidates_for_prompt(
        self,
        candidates: List[Tuple[str, str]],
        max_count: int = 40
    ) -> Tuple[List[str], Dict[str, str]]:
        """
        å°†å€™é€‰é“¾æ¥ç¼–ç ä¸º link_idï¼Œä¾›æ¨¡å‹é€‰æ‹©
        """
        lines: List[str] = []
        link_id_map: Dict[str, str] = {}

        for idx, (anchor, url) in enumerate(candidates[:max_count], start=1):
            link_id = f"L{idx}"
            clean_anchor = re.sub(r'\s+', ' ', (anchor or '').strip())
            label = clean_anchor[:80] if clean_anchor else "ï¼ˆæ— é”šæ–‡æœ¬ï¼‰"
            lines.append(f"- {link_id} | {label} | {url}")
            link_id_map[link_id] = url

        return lines, link_id_map

    def _normalize_link_id(self, raw_link_id: Any) -> str:
        """
        è§„èŒƒåŒ– link_idï¼ˆå…¼å®¹ L1 / l1 / 1ï¼‰
        """
        value = str(raw_link_id or '').strip().upper()
        if not value:
            return ""

        if value.isdigit():
            return f"L{int(value)}"

        match = re.match(r'^L(\d+)$', value)
        if not match:
            return ""
        return f"L{int(match.group(1))}"

    def _score_link_match(self, title: str, anchor: str, url: str) -> int:
        """æ ¹æ®æ ‡é¢˜ä¸å€™é€‰é“¾æ¥æ–‡æœ¬åŒ¹é…ç¨‹åº¦æ‰“åˆ†"""
        title_tokens = set(re.findall(r'[\u4e00-\u9fa5]{2,}|[A-Za-z0-9]{3,}', title.lower()))
        if not title_tokens:
            return 0
        haystack = f"{anchor} {url}".lower()
        score = 0
        for token in title_tokens:
            if token in haystack:
                score += 1
        return score

    def _select_item_link(
        self,
        item_title: str,
        candidates: List[Tuple[str, str]],
        used_urls: set,
        fallback_url: str,
        preferred_link_id: str = "",
        link_id_map: Optional[Dict[str, str]] = None
    ) -> str:
        """ä¸ºæ¡ç›®åˆ†é…æœ€åˆé€‚çš„é“¾æ¥"""
        normalized_link_id = self._normalize_link_id(preferred_link_id)
        if normalized_link_id and link_id_map:
            preferred_url = link_id_map.get(normalized_link_id, "")
            if preferred_url and preferred_url not in used_urls:
                used_urls.add(preferred_url)
                return preferred_url

        best_url = ""
        best_score = 0
        for anchor, url in candidates:
            if url in used_urls:
                continue
            score = self._score_link_match(item_title, anchor, url)
            if score > best_score:
                best_score = score
                best_url = url

        if best_url:
            used_urls.add(best_url)
            return best_url

        for _, url in candidates:
            if url not in used_urls:
                used_urls.add(url)
                return url

        return fallback_url

    def _is_bad_image_url(self, image_url: str) -> bool:
        """åˆ¤æ–­å›¾ç‰‡ URL æ˜¯å¦ä¸ºç«™ç‚¹è£…é¥°å›¾æˆ–æ— æ•ˆå›¾"""
        if not image_url:
            return True

        lower = image_url.lower()
        bad_keywords = (
            'logo',
            'avatar',
            'favicon',
            'icon',
            'sprite',
            'placeholder',
            'default',
            'wechat-qun',
            'qrcode',
            'qr-code',
        )
        if any(keyword in lower for keyword in bad_keywords):
            return True
        if lower.endswith('.svg') or lower.endswith('.ico'):
            return True
        return False

    def _fetch_page_preview_image(self, page_url: str) -> str:
        """
        ä»é¡µé¢æå–é¢„è§ˆå›¾ï¼ˆä¼˜å…ˆ og:imageï¼‰
        """
        if not page_url or not page_url.startswith('http'):
            return ""

        if page_url in self._page_image_cache:
            return self._page_image_cache[page_url]

        image_url = ""
        try:
            response = self.http_session.get(page_url, timeout=12)
            response.raise_for_status()

            from bs4 import BeautifulSoup
            soup = BeautifulSoup(response.text, 'html.parser')

            meta_candidates = [
                ('meta', {'property': 'og:image'}, 'content'),
                ('meta', {'property': 'og:image:url'}, 'content'),
                ('meta', {'name': 'twitter:image'}, 'content'),
                ('meta', {'itemprop': 'image'}, 'content'),
            ]
            for tag_name, attrs, attr_name in meta_candidates:
                tag = soup.find(tag_name, attrs=attrs)
                value = (tag.get(attr_name, '') if tag else '').strip()
                if not value:
                    continue
                candidate = urljoin(page_url, value)
                if not self._is_bad_image_url(candidate):
                    image_url = candidate
                    break

            if not image_url:
                first_img = soup.select_one('article img, main img, .content img, #content img, body img')
                if first_img:
                    src = (first_img.get('src') or first_img.get('data-src') or '').strip()
                    if src:
                        candidate = urljoin(page_url, src)
                        if not self._is_bad_image_url(candidate):
                            image_url = candidate
        except Exception as e:
            logger.debug(f"å›¾ç‰‡å›å¡«å¤±è´¥: {page_url}, é”™è¯¯: {e}")
            image_url = ""

        self._page_image_cache[page_url] = image_url
        return image_url

    def _resolve_item_image_url(self, item_url: str, source_url: str, fallback_image_url: str) -> str:
        """
        ä¸ºæ¡ç›®é€‰æ‹©å›¾ç‰‡ï¼šä¼˜å…ˆæ¡ç›®é¡µå›¾ç‰‡ï¼Œé¿å…èšåˆæºï¼ˆå¦‚å…¬ä¼—å·å°é¢ï¼‰é‡å¤å›¾
        """
        clean_item_url = (item_url or "").strip()
        clean_source_url = (source_url or "").strip()
        is_wechat_source = "mp.weixin.qq.com" in clean_source_url.lower()

        # 1. ä¼˜å…ˆä½¿ç”¨æ¡ç›®é“¾æ¥å¯¹åº”çš„å›¾ç‰‡ï¼ˆé¿å…å…¬ä¼—å·å°é¢å›¾å¤ç”¨ï¼‰
        if clean_item_url and clean_item_url != clean_source_url:
            image_url = self._fetch_page_preview_image(clean_item_url)
            if image_url and not self._is_bad_image_url(image_url):
                return image_url
            if is_wechat_source:
                # å¾®ä¿¡èšåˆæ–‡ç« å¸¸è§å°é¢é‡å¤ï¼Œæ¡ç›®é¡µæ‹¿ä¸åˆ°å›¾æ—¶å®ç¼ºæ¯‹æ»¥
                return ""

        # 2. å¯¹éå¾®ä¿¡æ¥æºï¼Œæ‰è€ƒè™‘ç›´æ¥ä½¿ç”¨åŸå§‹å›é€€å›¾
        if fallback_image_url and not self._is_bad_image_url(fallback_image_url):
            return fallback_image_url

        # 3. æœ€åå…œåº•å°è¯• source é¡µ
        for page_url in [clean_source_url]:
            if not page_url:
                continue
            image_url = self._fetch_page_preview_image(page_url)
            if image_url and not self._is_bad_image_url(image_url):
                return image_url
        return ""

    def _build_dedup_key(self, item_url: str, source_url: str, title: str) -> str:
        """
        æ„å»ºæ¡ç›®çº§å»é‡é”®ï¼šåŒæºé“¾æ¥æ—¶è¿½åŠ æ ‡é¢˜ï¼Œé¿å…èšåˆé¡µæ¡ç›®äº’ç›¸è¦†ç›–
        """
        dedup_key = (item_url or source_url or "").strip()
        if dedup_key and source_url and dedup_key == source_url:
            normalized_title = re.sub(r'\s+', '', str(title).lower())
            return f"{source_url}#{normalized_title[:80]}"
        return dedup_key

    def _parse_ai_items_response(self, response_text: str) -> List[Dict[str, Any]]:
        """
        è§£æ AI è¿”å›çš„ JSONï¼Œæ”¯æŒ {"items": [...]} å’Œ [...] ä¸¤ç§æ ¼å¼
        """
        clean_text = response_text or ""
        clean_text = re.sub(r'```json\s*', '', clean_text, flags=re.IGNORECASE)
        clean_text = re.sub(r'```\s*', '', clean_text)
        clean_text = re.sub(r'<thinking>.*?</thinking>', '', clean_text, flags=re.DOTALL)
        clean_text = re.sub(r'<thinking>.*', '', clean_text, flags=re.DOTALL)
        clean_text = clean_text.strip()

        if not clean_text:
            return []

        payloads = [clean_text]
        object_match = re.search(r'\{.*\}', clean_text, re.DOTALL)
        array_match = re.search(r'\[.*\]', clean_text, re.DOTALL)
        if object_match:
            payloads.append(object_match.group())
        if array_match:
            payloads.append(array_match.group())

        for payload in payloads:
            try:
                parsed = json.loads(payload)
                if isinstance(parsed, dict):
                    items = parsed.get('items', [])
                    if isinstance(items, list):
                        return items
                if isinstance(parsed, list):
                    return parsed
            except (json.JSONDecodeError, TypeError):
                continue
        return []
    
    def _extract_items(self, article: Article) -> List[Dict[str, Any]]:
        """
        ä½¿ç”¨ AI ä»æ–‡ç« ä¸­æå–å¤šä¸ªæ¡ç›®
        
        Args:
            article: æ–‡ç« å¯¹è±¡
            
        Returns:
            åŒ…å«å¤šä¸ªæ¡ç›®çš„åˆ—è¡¨ï¼Œæ¯ä¸ªæ¡ç›®æœ‰ title, summary, category, is_english
        """
        try:
            content = article.content or article.summary
            content = truncate_text(content, 8000)  # å¢åŠ å†…å®¹é•¿åº¦ä»¥è·å–æ›´å¤šä¿¡æ¯
            candidate_links = self._extract_candidate_links(article)
            candidate_link_lines, link_id_map = self._build_link_candidates_for_prompt(candidate_links)
            candidate_link_block = "\n".join(candidate_link_lines) if candidate_link_lines else "- æ— å¯ç”¨å€™é€‰é“¾æ¥ï¼ˆè¯·è¿”å›ç©º link_idï¼‰"
            used_item_urls = set()
            
            # è·å–æ‰€æœ‰å¯ç”¨åˆ†ç±»
            categories = self.config.get('categories', {})
            category_names = [cat.get('name', key) for key, cat in categories.items() if key != 'training']
            
            # æ£€æµ‹æ˜¯å¦ä¸ºæ—¥åˆŠ/èšåˆç±»å†…å®¹
            is_daily_digest = any(kw in article.title.lower() or kw in content[:500].lower() 
                                  for kw in ['æ—¥åˆŠ', 'æ—¥æŠ¥', 'ä»Šæ—¥æ‘˜è¦', 'æ¯æ—¥', 'daily', 'å‘¨åˆŠ'])
            
            if is_daily_digest:
                extract_prompt = f"""ä½ æ˜¯ä¸€ä¸ªæŠ€æœ¯èµ„è®¯ç¼–è¾‘åŠ©æ‰‹ã€‚

è¿™æ˜¯ä¸€ç¯‡æ—¥åˆŠ/æ—¥æŠ¥å†…å®¹ï¼ŒåŒ…å«å¤šæ¡ç‹¬ç«‹çš„èµ„è®¯ã€‚è¯·ä»ä¸­æå–æ¯ä¸€æ¡ç‹¬ç«‹çš„æ–°é—»/èµ„è®¯ã€‚

ã€å¯é€‰åˆ†ç±»ã€‘
{', '.join(category_names)}

ã€åˆ†ç±»æŒ‡å—ã€‘
- æ—¶äº‹ï¼šè¡Œä¸šåŠ¨æ€ã€æ”¿ç­–æ–°é—»ã€å…¬å¸èèµ„ã€å¸‚åœºè¶‹åŠ¿ã€äº§ä¸šè§„åˆ’ç­‰ç»¼åˆèµ„è®¯
- AIèµ„è®¯ï¼šAIæ¨¡å‹å‘å¸ƒã€AIäº§å“æ›´æ–°ã€AIæŠ€æœ¯çªç ´ç­‰ä¸AIç›´æ¥ç›¸å…³çš„èµ„è®¯
- æ•™ç¨‹ï¼šæŠ€æœ¯æ•™ç¨‹ã€å·¥ä½œæµåˆ†äº«ã€å­¦ä¹ èµ„æºã€æœ€ä½³å®è·µç­‰
- å·¥å…·ï¼šå¼€æºé¡¹ç›®ã€å¼€å‘å·¥å…·ã€å®ç”¨è½¯ä»¶ç­‰

ã€é‡è¦ã€‘è¿™æ˜¯èšåˆç±»æ—¥åˆŠå†…å®¹ï¼Œä½ å¿…é¡»ï¼š
1. å°†æ—¥åˆŠæ‹†åˆ†æˆç‹¬ç«‹çš„èµ„è®¯æ¡ç›®ï¼Œæ¯æ¡èµ„è®¯å•ç‹¬æå–
2. ä¸è¦æŠŠå¤šæ¡èµ„è®¯åˆå¹¶æˆä¸€ä¸ªæ¡ç›®
3. æ—¥åˆŠä¸­é€šå¸¸æœ‰"äº§å“ä¸åŠŸèƒ½æ›´æ–°"ã€"å‰æ²¿ç ”ç©¶"ã€"è¡Œä¸šå±•æœ›"ç­‰åˆ†ç±»ï¼Œæ¯ä¸ªåˆ†ç±»ä¸‹çš„æ¯ä¸€æ¡éƒ½æ˜¯ç‹¬ç«‹èµ„è®¯
4. æå–æ•°é‡ï¼š5-10æ¡æœ€é‡è¦çš„èµ„è®¯
5. åŠ¡å¿…æ ¹æ®å†…å®¹åˆç†åˆ†é…åˆ°ä¸åŒåˆ†ç±»ï¼Œä¸è¦éƒ½æ”¾åˆ°åŒä¸€åˆ†ç±»
6. æ¯æ¡èµ„è®¯å°½é‡é€‰æ‹©æœ€åŒ¹é…çš„ link_idï¼›å¦‚æœæ— æ³•åŒ¹é…ï¼Œlink_id è¿”å›ç©ºå­—ç¬¦ä¸²
7. summary å¿…é¡»ç”¨â€œç¼–è¾‘ç‚¹è¯„â€è¯­æ°”å†™ 2-3 å¥ï¼Œé¿å…ç…§æŠ„åŸæ–‡ï¼ŒåŒ…å« 2-4 ä¸ª emoji

ã€è¾“å‡ºæ ¼å¼ã€‘
å¿…é¡»è¾“å‡º JSON å¯¹è±¡ï¼Œä¸è¦ä»»ä½• markdown æ ‡è®°æˆ–é¢å¤–æ–‡æœ¬ï¼š
{{
  "items": [
    {{"title": "åŒ—äº¬AIäº§ä¸šä¸¤å¹´å†²ä¸‡äº¿", "summary": "åŒ—äº¬å‘å¸ƒä¹å¤§è¡ŒåŠ¨è®¡åˆ’ï¼Œæ ¸å¿ƒäº§ä¸šè§„æ¨¡é¢„è®¡ä»4500äº¿å†²åˆºä¸‡äº¿ï¼Œä¿¡å·å¾ˆå¼ºã€‚å¯¹åŒºåŸŸäº§ä¸šé“¾æ˜¯æ˜æ˜¾åˆ©å¥½ï¼Œå€¼å¾—æŒç»­è·Ÿè¸ªã€‚ğŸ“ˆğŸ™ï¸", "category": "æ—¶äº‹", "is_english": false, "link_id": "L3"}}
  ]
}}

å¦‚æœæ— æ³•æå–ï¼Œè¿”å› {{"items": []}}"""
            else:
                extract_prompt = f"""ä½ æ˜¯ä¸€ä¸ªå‰ç«¯æŠ€æœ¯å‘¨åˆŠç¼–è¾‘åŠ©æ‰‹ã€‚

ä»ä»¥ä¸‹æ–‡ç« å†…å®¹ä¸­æå–æ‰€æœ‰æœ‰ä»·å€¼çš„ç‹¬ç«‹èµ„è®¯æ¡ç›®ã€‚

ã€å¯é€‰åˆ†ç±»ã€‘
{', '.join(category_names)}

ã€æå–è§„åˆ™ã€‘
1. æ¯ä¸ªæ¡ç›®åªæè¿°ä¸€ä»¶å…·ä½“çš„äº‹ï¼Œä¸è¦èšåˆ
2. ä¸ºæ¯ä¸ªæ¡ç›®é€‰æ‹©æœ€åˆé€‚çš„åˆ†ç±»
3. å¦‚æœæ–‡ç« æ˜¯æ—¥åˆŠ/å‘¨åˆŠåˆé›†ï¼Œæå–å…¶ä¸­æ‰€æœ‰é‡è¦çš„ç‹¬ç«‹èµ„è®¯ï¼ˆæœ€å¤š10æ¡ï¼‰
4. å¦‚æœæ–‡ç« åªåŒ…å«å•ä¸€ä¸»é¢˜ï¼Œåªè¿”å›1æ¡
5. è¿‡æ»¤æ‰å¹¿å‘Šã€æ‹›è˜ç­‰æ— å…³å†…å®¹
6. æ¯æ¡èµ„è®¯å°½é‡é€‰æ‹©æœ€åŒ¹é…çš„ link_idï¼›å¦‚æœæ— æ³•åŒ¹é…ï¼Œlink_id è¿”å›ç©ºå­—ç¬¦ä¸²
7. summary å¿…é¡»ç”¨â€œç¼–è¾‘ç‚¹è¯„â€è¯­æ°”å†™ 2-3 å¥ï¼Œé¿å…ç…§æŠ„åŸæ–‡ï¼ŒåŒ…å« 2-4 ä¸ª emoji

ã€è¾“å‡ºæ ¼å¼ã€‘
å¿…é¡»è¾“å‡º JSON å¯¹è±¡ï¼Œä¸è¦ä»»ä½• markdown æ ‡è®°æˆ–é¢å¤–æ–‡æœ¬ï¼š
{{
  "items": [
    {{"title": "15å­—ä»¥å†…çš„ä¸­æ–‡æ ‡é¢˜", "summary": "å…ˆè¯´æ¸…äº‹ä»¶ï¼Œå†è¡¥ä¸€ä¸¤å¥ç‚¹è¯„ï¼ŒåŒ…å«emojiã€‚ğŸš€âœ¨", "category": "ä»å¯é€‰åˆ†ç±»ä¸­é€‰æ‹©ä¸€ä¸ª", "is_english": false, "link_id": "L1"}}
  ]
}}

å¦‚æœæ²¡æœ‰å¯æå–çš„å†…å®¹ï¼Œè¿”å› {{"items": []}}"""
            
            user_prompt = f"""æ ‡é¢˜ï¼š{article.title}
æ¥æºï¼š{article.source}
URLï¼š{article.url}

å€™é€‰é“¾æ¥ï¼ˆåªèƒ½è¿”å› link_idï¼Œä¸è¦è¿”å› URLï¼‰ï¼š
{candidate_link_block}

å†…å®¹ï¼š
{content}"""
            
            # æ—¥åˆŠç±»å†…å®¹éœ€è¦æ›´å¤š token æ¥è¾“å‡ºå¤šä¸ªæ¡ç›®
            max_tokens = 4000 if is_daily_digest else 2000
            if self.ai_max_tokens:
                max_tokens = min(max_tokens, self.ai_max_tokens)
            logger.info(f"  æ—¥åˆŠæ£€æµ‹: {is_daily_digest}, æ–‡ç« : {article.title[:30]}...")
            
            response = self.ai_client.chat.completions.create(
                model=self.ai_model,
                messages=[
                    {"role": "system", "content": extract_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                max_tokens=max_tokens,
                temperature=0.2
            )
            
            response_text = response.choices[0].message.content
            logger.debug(f"  AIåŸå§‹å“åº”(å‰300å­—): {response_text[:300] if response_text else 'None'}...")
            
            items = self._parse_ai_items_response(response_text)

            # æ¸…ç†å¹¶è¿”å›æ¡ç›®
            result = []
            for item in items:
                if not isinstance(item, dict):
                    continue
                title = str(item.get('title', '')).strip()
                summary = self._format_editor_summary(str(item.get('summary', '')))
                if not title or summary == "æš‚æ— æè¿°":
                    continue

                preferred_link_id = self._normalize_link_id(item.get('link_id', ''))
                model_item_url = str(item.get('item_url', '') or item.get('url', '')).strip()
                if model_item_url.startswith('http') and model_item_url not in used_item_urls:
                    item_url = model_item_url
                    used_item_urls.add(model_item_url)
                else:
                    item_url = self._select_item_link(
                        title,
                        candidate_links,
                        used_item_urls,
                        article.url,
                        preferred_link_id=preferred_link_id,
                        link_id_map=link_id_map
                    )

                result.append({
                    "title": title,
                    "summary": summary,
                    "category": str(item.get('category', 'æ—¶äº‹')).strip() or "æ—¶äº‹",
                    "is_english": bool(item.get('is_english', self._detect_english(title))),
                    "source_url": article.url,
                    "item_url": item_url,
                    "image_url": article.image_url
                })

            if result:
                logger.info(f"  æˆåŠŸæå– {len(result)} ä¸ªæ¡ç›®")
                return result

            logger.warning("  AI ç»“æœè§£æåæ²¡æœ‰æœ‰æ•ˆæ¡ç›®ï¼Œä½¿ç”¨å›é€€æ¨¡å¼")
            
            # è§£æå¤±è´¥ï¼Œè¿”å›å•æ¡ç›®ï¼ˆå…¼å®¹åŸé€»è¾‘ï¼‰
            # å¯¹äºæ—¥åˆŠç±»å†…å®¹ï¼Œå°è¯•ä»å†…å®¹ä¸­æå–æœ‰æ„ä¹‰çš„æ ‡é¢˜å’Œç®€ä»‹
            fallback_title = self._extract_fallback_title(article)
            fallback_summary = self._format_editor_summary(
                self._extract_fallback_summary(article, fallback_title)
            )
            return [{
                "title": fallback_title,
                "summary": fallback_summary,
                "category": "AIèµ„è®¯" if is_daily_digest else "æ—¶äº‹",
                "is_english": self._detect_english(article.title),
                "source_url": article.url,
                "item_url": self._select_item_link(
                    fallback_title,
                    candidate_links,
                    used_item_urls,
                    article.url,
                    link_id_map=link_id_map
                ),
                "image_url": article.image_url
            }]
            
        except Exception as e:
            logger.error(f"æå–æ¡ç›®å¤±è´¥: {article.title}, é”™è¯¯: {e}")
            fallback_title = self._extract_fallback_title(article)
            fallback_summary = self._format_editor_summary(
                self._extract_fallback_summary(article, fallback_title)
            )
            return [{
                "title": fallback_title,
                "summary": fallback_summary,
                "category": "AIèµ„è®¯",
                "is_english": self._detect_english(article.title),
                "source_url": article.url,
                "item_url": article.url,
                "image_url": article.image_url
            }]
    
    def _clean_summary(self, summary: str) -> str:
        """
        æ¸…ç†æ‘˜è¦å†…å®¹ï¼Œç§»é™¤æ— æ•ˆä¿¡æ¯
        
        Args:
            summary: åŸå§‹æ‘˜è¦
            
        Returns:
            æ¸…ç†åçš„æ‘˜è¦
        """
        if not summary:
            return "æš‚æ— æè¿°"
        
        # éœ€è¦è¿‡æ»¤çš„æ— æ•ˆå†…å®¹æ¨¡å¼
        invalid_patterns = [
            r'Article URL:\s*<[^>]+>',
            r'Comments URL:\s*<[^>]+>',
            r'Points:\s*\d+',
            r'# Comments:\s*\d+',
            r'Comments:\s*\d+',
            r'<https?://[^>]+>',  # å°–æ‹¬å·åŒ…è£¹çš„ URL
            r'Article URL:.*',
            r'Comments URL:.*',
            r'<thinking>.*?</thinking>',  # ç§»é™¤ thinking æ ‡ç­¾åŠå…¶å†…å®¹
            r'<thinking>.*',  # ç§»é™¤æœªé—­åˆçš„ thinking æ ‡ç­¾
        ]
        
        clean_text = summary
        for pattern in invalid_patterns:
            clean_text = re.sub(pattern, '', clean_text, flags=re.IGNORECASE | re.MULTILINE | re.DOTALL)
        
        # ç§»é™¤å¤šä½™çš„ç©ºè¡Œå’Œç©ºæ ¼
        clean_text = re.sub(r'\n\s*\n', '\n', clean_text)
        clean_text = clean_text.strip()
        
        # å¦‚æœæ¸…ç†åå†…å®¹å¤ªçŸ­æˆ–ä¸ºç©ºï¼Œè¿”å›é»˜è®¤å€¼
        if not clean_text or len(clean_text) < 10:
            return "æš‚æ— æè¿°"
        
        return clean_text

    def _format_editor_summary(self, summary: str) -> str:
        """
        å°†æ‘˜è¦æ•´ç†ä¸ºå¸¦è½»ç‚¹è¯„çš„ç¼–è¾‘å£å»ï¼Œå¹¶è¡¥å…… emoji é£æ ¼
        """
        clean_text = self._clean_summary(summary)
        if clean_text == "æš‚æ— æè¿°":
            return clean_text

        clean_text = re.sub(r'^\s*\d+\.\s*', '', clean_text)
        clean_text = re.sub(r'\s+', ' ', clean_text).strip()

        if len(clean_text) > 180:
            clean_text = clean_text[:180].rstrip('ï¼Œ,ï¼›;ã€‚.!?ï¼ï¼Ÿ')

        emoji_pattern = r'[\U0001F300-\U0001FAFF\u2600-\u27BF]'
        emoji_count = len(re.findall(emoji_pattern, clean_text))
        if emoji_count == 0:
            if not clean_text.endswith(('ã€‚', 'ï¼', 'ï¼Ÿ', '.', '!', '?')):
                clean_text += 'ã€‚'
            clean_text += ' ğŸ”âœ¨'
        elif emoji_count == 1:
            clean_text += ' ğŸš€'

        return clean_text

    def _fetch_github_trending_tools(self, limit: int = 20) -> List[Dict[str, str]]:
        """
        æŠ“å– GitHub Trendingï¼Œç”¨äºå·¥å…·åˆ†ç±»å…œåº•è¡¥å…¨
        """
        try:
            response = self.http_session.get("https://github.com/trending?since=daily", timeout=15)
            response.raise_for_status()

            from bs4 import BeautifulSoup
            soup = BeautifulSoup(response.text, 'html.parser')
            repos: List[Dict[str, str]] = []

            for row in soup.select('article.Box-row'):
                repo_link = row.select_one('h2 a[href]')
                if not repo_link:
                    continue

                href = (repo_link.get('href') or '').strip()
                if not href:
                    continue
                repo_url = urljoin("https://github.com", href)
                repo_name = re.sub(r'\s+', '', repo_link.get_text(" ", strip=True)).strip('/')
                if not repo_name:
                    continue

                desc_node = row.select_one('p')
                desc = desc_node.get_text(" ", strip=True) if desc_node else ""
                star_node = row.select_one('a[href$="/stargazers"]')
                stars = star_node.get_text(" ", strip=True) if star_node else ""

                repos.append({
                    "name": repo_name,
                    "url": repo_url,
                    "description": desc,
                    "stars": stars,
                })
                if len(repos) >= limit:
                    break
            return repos
        except Exception as e:
            logger.warning(f"æŠ“å– GitHub Trending å¤±è´¥: {e}")
            return []

    def _build_tool_fallback_summary(self, name: str, description: str, stars: str) -> str:
        """
        ç”Ÿæˆå·¥å…·è¡¥å…¨é¡¹çš„ç¼–è¾‘ç‚¹è¯„æ‘˜è¦
        """
        desc = re.sub(r'\s+', ' ', (description or "").strip())
        if len(desc) > 100:
            desc = desc[:100].rstrip('ï¼Œ,ï¼›;ã€‚.!?ï¼ï¼Ÿ:ï¼š') + "..."
        stars_text = f"ï¼Œå½“å‰çƒ­åº¦ {stars}" if stars else ""
        if desc:
            summary = f"ğŸš€ GitHub çƒ­é—¨é¡¹ç›® {name}{stars_text}ï¼š{desc}ã€‚å»ºè®®å…ˆçœ‹ README ä¸æœ€è¿‘æäº¤ï¼Œå†è¯„ä¼°æ˜¯å¦å¼•å…¥åˆ°ä½ çš„å·¥ä½œæµã€‚â­ğŸ› ï¸"
        else:
            summary = f"ğŸš€ GitHub çƒ­é—¨é¡¹ç›® {name}{stars_text}ï¼Œè¿‘æœŸå…³æ³¨åº¦å¾ˆé«˜ã€‚å»ºè®®å¿«é€Ÿæµè§ˆ READMEã€Issue ä¸ç¤ºä¾‹ï¼Œåˆ¤æ–­æ˜¯å¦é€‚åˆå½“å‰ä¸šåŠ¡ã€‚â­ğŸ› ï¸"
        return self._format_editor_summary(summary)

    def _compose_editor_commentary(self, title: str, raw_summary: str, category_name: str) -> str:
        """
        ç”¨æ¨¡å‹ç”Ÿæˆç®€çŸ­ç‚¹è¯„ï¼Œå¤±è´¥æ—¶é€€å›æœ¬åœ°æ‘˜è¦æ ¼å¼åŒ–
        """
        base_summary = self._clean_summary(raw_summary) if raw_summary else "æš‚æ— æè¿°"
        if base_summary == "æš‚æ— æè¿°":
            return base_summary

        prompt = f"""ä½ æ˜¯æŠ€æœ¯å‘¨åˆŠç¼–è¾‘ã€‚è¯·åŸºäºç»™å®šæ ‡é¢˜å’Œç´ æï¼Œå†™ä¸€æ®µä¸­æ–‡ç‚¹è¯„ã€‚

è¦æ±‚ï¼š
1. 2-3å¥ï¼Œæ€»é•¿åº¦çº¦70-130å­—
2. ä¸è¦ç…§æŠ„ç´ æåŸå¥ï¼Œè¦æœ‰ç¼–è¾‘è§†è§’
3. åŒ…å«2-4ä¸ªemoji
4. ä¸è¦è¾“å‡ºæ ‡é¢˜ï¼Œä¸è¦markdownï¼Œä»…è¾“å‡ºä¸€æ®µæ­£æ–‡

åˆ†ç±»ï¼š{category_name}
æ ‡é¢˜ï¼š{title}
ç´ æï¼š{truncate_text(base_summary, 320)}"""
        try:
            response = self.ai_client.chat.completions.create(
                model=self.ai_model,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸“ä¸šçš„æŠ€æœ¯ç¼–è¾‘ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=220,
                temperature=0.5
            )
            content = response.choices[0].message.content if response.choices else ""
            if content:
                return self._format_editor_summary(content)
        except Exception as e:
            logger.debug(f"ç¼–è¾‘ç‚¹è¯„ç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨æœ¬åœ°å›é€€: {title[:40]}..., é”™è¯¯: {e}")

        return self._format_editor_summary(base_summary)

    def _get_fallback_feeds_for_category(self, category_name: str) -> List[Dict[str, str]]:
        """
        åˆ†ç±»å…œåº•çš„è”ç½‘æ¥æºï¼ˆRSSï¼‰
        """
        if category_name == "AIèµ„è®¯":
            return [
                {"name": "OpenAI News", "url": "https://openai.com/news/rss.xml"},
                {"name": "Hugging Face Blog", "url": "https://huggingface.co/blog/feed.xml"},
                {"name": "Google AI Blog", "url": "https://blog.google/technology/ai/rss/"},
                {"name": "VentureBeat AI", "url": "https://venturebeat.com/category/ai/feed/"},
                {"name": "AI News", "url": "https://www.artificialintelligence-news.com/feed/"},
                {"name": "MIT AI Topic", "url": "https://www.technologyreview.com/topic/artificial-intelligence/feed"},
            ]

        if category_name == "æ—¶äº‹":
            return [
                {"name": "TechCrunch", "url": "https://techcrunch.com/feed/"},
                {"name": "The Verge", "url": "https://www.theverge.com/rss/index.xml"},
                {"name": "InfoQ", "url": "https://www.infoq.com/feed/"},
                {"name": "36Kr", "url": "https://www.36kr.com/feed"},
            ]

        if category_name == "æ•™ç¨‹":
            return [
                {"name": "Frontend Masters Blog", "url": "https://frontendmasters.com/blog/feed/"},
                {"name": "CSS-Tricks", "url": "https://css-tricks.com/feed/"},
                {"name": "Smashing Magazine", "url": "https://www.smashingmagazine.com/feed/"},
                {"name": "web.dev", "url": "https://web.dev/feed.xml"},
            ]

        return []

    def _article_timestamp(self, article: Article) -> float:
        """
        ç»Ÿä¸€æ–‡ç« æ—¶é—´æˆ³ï¼Œä¾¿äºæ’åº
        """
        if not article.published:
            return 0.0
        dt = article.published
        if dt.tzinfo is None:
            dt = dt.replace(tzinfo=timezone.utc)
        return dt.timestamp()

    def _collect_fallback_articles(
        self,
        feeds: List[Dict[str, str]],
        max_articles: int = 120
    ) -> List[Article]:
        """
        ä»å…œåº• RSS æºæŠ“å–å¹¶æŒ‰æ—¶é—´æ’åº
        """
        if not feeds:
            return []

        fetcher = RSSFetcher(feeds)
        articles = fetcher.fetch_all()
        if not articles:
            return []

        # å…œåº•é˜¶æ®µé€‚å½“æ”¾å®½æ—¶é—´çª—å£ï¼Œä¿è¯æœ€å°æ•°é‡ç›®æ ‡
        fallback_hours = max(self.config.get('time_filter', {}).get('hours', 168), 336)
        now = datetime.now(timezone.utc).timestamp()
        cutoff = now - fallback_hours * 3600

        filtered = []
        for article in articles:
            ts = self._article_timestamp(article)
            if ts == 0 or ts >= cutoff:
                filtered.append(article)

        filtered.sort(key=self._article_timestamp, reverse=True)
        return filtered[:max_articles]

    def _supplement_category_with_feeds(
        self,
        category_name: str,
        needed_count: int,
        run_dedup_urls: set,
        used_image_urls: set
    ) -> List[WeeklyItem]:
        """
        é€šè¿‡è”ç½‘ RSS å…œåº•è¡¥é½æŒ‡å®šåˆ†ç±»
        """
        if needed_count <= 0:
            return []

        feeds = self._get_fallback_feeds_for_category(category_name)
        fallback_articles = self._collect_fallback_articles(feeds, max_articles=max(needed_count * 20, 80))
        if not fallback_articles:
            return []

        items: List[WeeklyItem] = []
        for article in fallback_articles:
            title = (article.title or "").strip()
            item_url = (article.url or "").strip()
            if not title or not item_url:
                continue

            dedup_key = self._build_dedup_key(item_url, item_url, title)
            if dedup_key in run_dedup_urls:
                continue
            if self.deduplicator and self.deduplicator.is_duplicate(dedup_key):
                continue

            raw_summary = article.summary or article.content or title
            summary = self._compose_editor_commentary(title, raw_summary, category_name)
            if not summary or summary == "æš‚æ— æè¿°":
                continue

            image_url = self._resolve_item_image_url(item_url, item_url, article.image_url)
            if image_url and image_url in used_image_urls:
                image_url = ""
            if image_url:
                used_image_urls.add(image_url)

            run_dedup_urls.add(dedup_key)
            items.append(WeeklyItem(
                title=title,
                url=item_url,
                summary=summary,
                is_english=self._detect_english(title),
                category=category_name,
                short_title=title,
                image_url=image_url,
                item_url=item_url,
                source_url=item_url
            ))

            if len(items) >= needed_count:
                break

        if items:
            logger.info(f"{category_name} åˆ†ç±»å·²é€šè¿‡è”ç½‘å…œåº•è¡¥é½ {len(items)} æ¡")
        return items

    def _get_effective_min_count(self, category_name: str, config_min_count: int) -> int:
        """
        è®¡ç®—åˆ†ç±»æœ€å°æ•°é‡çº¦æŸï¼ˆæ—¶äº‹/AIèµ„è®¯å¼ºåˆ¶è‡³å°‘5ï¼‰
        """
        min_count = max(0, int(config_min_count or 0))
        if category_name in ("æ—¶äº‹", "AIèµ„è®¯"):
            min_count = max(min_count, 5)
        return min_count

    def _supplement_tools_with_github(
        self,
        needed_count: int,
        run_dedup_urls: set
    ) -> List[WeeklyItem]:
        """
        å½“å·¥å…·æ•°é‡ä¸è¶³æ—¶ï¼Œä½¿ç”¨ GitHub Trending è‡ªåŠ¨è¡¥é½
        """
        if needed_count <= 0:
            return []

        repos = self._fetch_github_trending_tools(limit=max(needed_count * 4, 20))
        if not repos:
            return []

        items: List[WeeklyItem] = []
        for repo in repos:
            title = repo.get("name", "").strip()
            item_url = repo.get("url", "").strip()
            if not title or not item_url:
                continue

            dedup_key = self._build_dedup_key(item_url, item_url, title)
            if dedup_key in run_dedup_urls:
                continue
            if self.deduplicator and self.deduplicator.is_duplicate(dedup_key):
                continue

            run_dedup_urls.add(dedup_key)
            item = WeeklyItem(
                title=title,
                url=item_url,
                summary=self._build_tool_fallback_summary(
                    title,
                    repo.get("description", ""),
                    repo.get("stars", "")
                ),
                is_english=self._detect_english(title),
                category="å·¥å…·",
                short_title=title,
                image_url="",
                item_url=item_url,
                source_url=item_url
            )
            items.append(item)
            if len(items) >= needed_count:
                break

        if items:
            logger.info(f"å·¥å…·åˆ†ç±»å·²ç”± GitHub Trending è¡¥é½ {len(items)} æ¡")
        return items
    
    def _extract_fallback_title(self, article: Article) -> str:
        """
        ä»æ—¥åˆŠç±»æ–‡ç« å†…å®¹ä¸­æå–æœ‰æ„ä¹‰çš„æ ‡é¢˜
        
        å½“AIæå–å¤±è´¥æ—¶ï¼Œå¦‚æœåŸå§‹æ ‡é¢˜æ˜¯æ—¥æœŸæ ¼å¼ï¼ˆå¦‚"2026-01-01æ—¥åˆŠ"ï¼‰ï¼Œ
        å°è¯•ä»å†…å®¹ä¸­æå–ç¬¬ä¸€æ¡æœ‰ä»·å€¼çš„èµ„è®¯æ ‡é¢˜ã€‚
        
        Args:
            article: æ–‡ç« å¯¹è±¡
            
        Returns:
            æå–çš„æ ‡é¢˜ï¼ˆ15å­—ä»¥å†…ï¼‰
        """
        original_title = article.title.strip()
        
        # æ£€æµ‹æ˜¯å¦ä¸ºæ—¥æœŸæ ¼å¼çš„æ—¥åˆŠ/æ—¥æŠ¥æ ‡é¢˜
        date_pattern = r'^\d{4}-?\d{2}-?\d{2}.*?(æ—¥åˆŠ|æ—¥æŠ¥|Daily)'
        is_date_title = re.match(date_pattern, original_title, re.IGNORECASE)
        
        if not is_date_title:
            # ä¸æ˜¯æ—¥æœŸæ ¼å¼ï¼Œç›´æ¥è¿”å›åŸæ ‡é¢˜ï¼ˆæˆªæ–­åˆ°15å­—ï¼‰
            return original_title[:15]
        
        # å°è¯•ä»å†…å®¹ä¸­æå–æœ‰æ„ä¹‰çš„æ ‡é¢˜
        content = article.content or article.summary or ""
        
        # ä¼˜å…ˆä»"ä»Šæ—¥æ‘˜è¦"åæå–ç¬¬ä¸€æ¡æœ‰æ„ä¹‰çš„èµ„è®¯
        # æ ¼å¼é€šå¸¸æ˜¯ï¼šä»Šæ—¥æ‘˜è¦ è±†åŒ…çœ¼é•œ2000å†…å”®è…¾è®¯imaä¸€é”®ç”ŸæˆPPT SeedFoldè¶…...
        summary_match = re.search(r'ä»Šæ—¥æ‘˜è¦\s*([^\n]{5,80})', content)
        if summary_match:
            summary_text = summary_match.group(1).strip()
            # æå–ç¬¬ä¸€ä¸ªæœ‰æ„ä¹‰çš„çŸ­è¯­ï¼ˆé€šå¸¸ä»¥ä¸­æ–‡åè¯/äº§å“å¼€å¤´ï¼‰
            # åŒ¹é…æ¨¡å¼ï¼šäº§å“å+åŠ¨ä½œï¼Œæˆ–è€…å…¬å¸å+äº§å“
            news_patterns = [
                # å…¬å¸/äº§å“å + åŠ¨ä½œï¼ˆå¦‚ï¼šè±†åŒ…çœ¼é•œå¼€å”®ã€è…¾è®¯imaä¸€é”®ç”ŸæˆPPTï¼‰
                r'([A-Za-z\u4e00-\u9fa5]{2,8}(?:çœ¼é•œ|æ¨¡å‹|å¹³å°|å·¥å…·|æ¡†æ¶|ç³»ç»Ÿ)?(?:å¼€å”®|å‘å¸ƒ|ä¸Šçº¿|å¼€æº|æ¨å‡º|è·å¾—|å®Œæˆ|èèµ„|çªç ´|è¶…è¶Š|å†²åˆº)[^\s]*)',
                # äº§å“ç‰ˆæœ¬æ ¼å¼ï¼ˆå¦‚ï¼šSeedFoldè¶…AlphaFold3ï¼‰
                r'([A-Za-z][A-Za-z0-9]{1,10}è¶…[A-Za-z0-9]{2,12})',
                # é€šç”¨çš„"XX+åŠ¨è¯"æ ¼å¼
                r'([\u4e00-\u9fa5A-Za-z]{2,6}(?:AI|çœ¼é•œ|æ¨¡å‹|èŠ¯ç‰‡|å¹³å°)?[\u4e00-\u9fa5]{2,8})',
            ]
            
            for pattern in news_patterns:
                match = re.search(pattern, summary_text)
                if match:
                    extracted = match.group(1).strip()
                    if 4 <= len(extracted) <= 15:
                        return extracted
            
            # å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°ï¼Œå–æ‘˜è¦çš„å‰15å­—
            return summary_text[:15]
        
        # å¤‡ç”¨ï¼šä»å†…å®¹ä¸­åŒ¹é…å¸¸è§çš„èµ„è®¯æ¨¡å¼
        patterns = [
            # ä¸­æ–‡äº§å“/å…¬å¸å + åŠ¨ä½œ
            r'([\u4e00-\u9fa5A-Za-z]{2,8}(?:å…¬æµ‹|å‘å¸ƒ|å¼€æº|ä¸Šçº¿|æ¨å‡º|å¼€å”®|å¼€æ”¾|è·å¾—|å®Œæˆ|å®£å¸ƒ|èèµ„|çªç ´)[^\nã€‚ï¼]{0,8})',
            # å…¬å¸å + äº§å“åŠ¨ä½œ
            r'((?:å°ç±³|å­—èŠ‚|è…¾è®¯|é˜¿é‡Œ|ç™¾åº¦|åä¸º|OpenAI|Meta|Google|å¾®è½¯|Apple|åŒ—äº¬|ä¸Šæµ·)[A-Za-z\u4e00-\u9fa5]{2,12})',
        ]
        
        for pattern in patterns:
            match = re.search(pattern, content[:2000])
            if match:
                extracted = match.group(1).strip()
                extracted = re.sub(r'\s+', '', extracted)
                if 4 <= len(extracted) <= 15:
                    return extracted
        
        # æœ€åå›é€€ï¼šä½¿ç”¨åŸæ ‡é¢˜
        return original_title[:15]
    
    def _extract_fallback_summary(self, article: Article, title: str) -> str:
        """
        ä»æ—¥åˆŠç±»æ–‡ç« å†…å®¹ä¸­æå–ä¸æ ‡é¢˜ç›¸å…³çš„ç®€ä»‹
        
        å½“AIæå–å¤±è´¥æ—¶ï¼Œå°è¯•ä»å†…å®¹ä¸­æ‰¾åˆ°ä¸æ ‡é¢˜ç›¸å…³çš„æè¿°ã€‚
        
        Args:
            article: æ–‡ç« å¯¹è±¡
            title: å·²æå–çš„æ ‡é¢˜
            
        Returns:
            æå–çš„ç®€ä»‹ï¼ˆçº¦100å­—ï¼‰
        """
        content = article.content or article.summary or ""
        
        # å°è¯•ä»å†…å®¹ä¸­æŸ¥æ‰¾ä¸æ ‡é¢˜ç›¸å…³çš„æ®µè½
        # æŸ¥æ‰¾æ ‡é¢˜é™„è¿‘çš„å†…å®¹
        title_keywords = list(filter(lambda x: len(x) >= 2, 
                                     re.findall(r'[A-Za-z]+|\u4e00-\u9fa5{2,}', title)))
        
        if title_keywords:
            # å°è¯•æ‰¾åˆ°åŒ…å«æ ‡é¢˜å…³é”®è¯çš„å¥å­
            for keyword in title_keywords[:3]:
                # æŸ¥æ‰¾å…³é”®è¯æ‰€åœ¨çš„å¥å­
                pattern = rf'[^ã€‚ï¼ï¼Ÿ\n]*{re.escape(keyword)}[^ã€‚ï¼ï¼Ÿ\n]*[ã€‚ï¼ï¼Ÿ]?'
                match = re.search(pattern, content)
                if match:
                    sentence = match.group().strip()
                    # é™åˆ¶é•¿åº¦å¹¶æ¸…ç†
                    if 20 <= len(sentence) <= 150:
                        return self._clean_summary(sentence)
        
        # å¦‚æœæ‰¾ä¸åˆ°ç›¸å…³å†…å®¹ï¼Œæå–"ä»Šæ—¥æ‘˜è¦"åçš„ä¸€æ®µå†…å®¹
        summary_pattern = r'ä»Šæ—¥æ‘˜è¦\s*(.{20,200})'
        summary_match = re.search(summary_pattern, content)
        if summary_match:
            return self._clean_summary(summary_match.group(1)[:150])
        
        # æœ€åå›é€€ï¼šä½¿ç”¨åŸå§‹ç®€ä»‹çš„å‰150å­—
        if article.summary:
            return self._clean_summary(article.summary[:150])
        
        return "æš‚æ— æè¿°"
    
    def _detect_english(self, text: str) -> bool:
        """
        æ£€æµ‹æ–‡æœ¬æ˜¯å¦ä¸ºè‹±æ–‡
        
        Args:
            text: å¾…æ£€æµ‹æ–‡æœ¬
            
        Returns:
            æ˜¯å¦ä¸ºè‹±æ–‡
        """
        if not text:
            return False
        # è®¡ç®—è‹±æ–‡å­—ç¬¦å æ¯”
        english_chars = sum(1 for c in text if c.isalpha() and ord(c) < 128)
        total_chars = sum(1 for c in text if c.isalpha())
        if total_chars == 0:
            return False
        return english_chars / total_chars > 0.7
    
    def _process_all_articles(self) -> Dict[str, List[WeeklyItem]]:
        """
        å¤„ç†æ‰€æœ‰æ–‡ç« ï¼Œç»Ÿä¸€æå–å¹¶æŒ‰åˆ†ç±»å½’ç±»
        
        Returns:
            æŒ‰åˆ†ç±»ååˆ†ç»„çš„ WeeklyItem å­—å…¸
        """
        categories_config = self.config.get('categories', {})
        all_items: Dict[str, List[WeeklyItem]] = {}
        processed_urls = set()
        run_dedup_urls = set()
        used_image_urls = set()
        allowed_category_names = {
            cat_config.get('name', cat_key)
            for cat_key, cat_config in categories_config.items()
            if cat_key != 'training'
        }
        
        # æ”¶é›†æ‰€æœ‰å”¯ä¸€çš„æ–‡ç« 
        all_articles = []
        for cat_key, cat_config in categories_config.items():
            if cat_key == 'training':
                continue
            articles = self._fetch_category_articles(cat_config)
            for article in articles:
                if article.url not in processed_urls:
                    all_articles.append(article)
                    processed_urls.add(article.url)
        
        logger.info(f"å…±æ”¶é›† {len(all_articles)} ç¯‡å”¯ä¸€æ–‡ç« ")
        
        # å¤„ç†æ¯ç¯‡æ–‡ç« ï¼Œæå–å¤šæ¡ç›®
        for article in all_articles:
            logger.info(f"  å¤„ç†æ–‡ç« : {article.title[:40]}...")
            
            extracted_items = self._extract_items(article)
            logger.info(f"    æå–åˆ° {len(extracted_items)} ä¸ªæ¡ç›®")
            
            for item_data in extracted_items:
                category = item_data.get('category', 'æ—¶äº‹')
                if category not in allowed_category_names:
                    category = "æ—¶äº‹"

                title = item_data.get('title', article.title)
                item_url = item_data.get('item_url') or article.url
                source_url = item_data.get('source_url') or article.url
                dedup_key = self._build_dedup_key(item_url, source_url, title)

                # ç¡®ä¿åˆ†ç±»å­˜åœ¨
                if category not in all_items:
                    all_items[category] = []

                if dedup_key in run_dedup_urls:
                    continue
                if self.deduplicator and self.deduplicator.is_duplicate(dedup_key):
                    logger.info(f"    è·³è¿‡å·²å¤„ç†æ¡ç›®: {item_data.get('title', '')[:40]}")
                    continue
                run_dedup_urls.add(dedup_key)

                image_url = self._resolve_item_image_url(
                    item_url,
                    source_url,
                    item_data.get('image_url', '')
                )
                if image_url and image_url in used_image_urls:
                    # é¿å…å‘¨åˆŠä¸­å¤§é¢ç§¯å¤ç”¨åŒä¸€å°é¢å›¾
                    if item_url and item_url != source_url:
                        alt_image = self._fetch_page_preview_image(item_url)
                        if alt_image and not self._is_bad_image_url(alt_image) and alt_image not in used_image_urls:
                            image_url = alt_image
                        else:
                            image_url = ""
                    else:
                        image_url = ""
                if image_url:
                    used_image_urls.add(image_url)
                
                item = WeeklyItem(
                    title=title,
                    url=item_url,
                    summary=item_data.get('summary', 'æš‚æ— æè¿°'),
                    is_english=item_data.get('is_english', False),
                    category=category,
                    short_title=item_data.get('title', ''),
                    image_url=image_url,
                    item_url=item_url,
                    source_url=source_url
                )
                all_items[category].append(item)

        # è”ç½‘å…œåº•ï¼šç¡®ä¿åˆ†ç±»è¾¾åˆ°æœ€å°æ•°é‡
        for cat_key, cat_config in categories_config.items():
            if cat_key == 'training':
                continue
            cat_name = cat_config.get('name', cat_key)
            min_count = self._get_effective_min_count(cat_name, cat_config.get('min_count', 0))
            current_count = len(all_items.get(cat_name, []))
            if current_count >= min_count:
                continue

            needed_count = min_count - current_count
            if cat_name == "å·¥å…·":
                fallback_items = self._supplement_tools_with_github(needed_count, run_dedup_urls)
            else:
                fallback_items = self._supplement_category_with_feeds(
                    cat_name,
                    needed_count,
                    run_dedup_urls,
                    used_image_urls
                )

            if fallback_items:
                if cat_name not in all_items:
                    all_items[cat_name] = []
                all_items[cat_name].extend(fallback_items)
        
        # æŒ‰é…ç½®çš„ max_count é™åˆ¶æ¯ä¸ªåˆ†ç±»çš„æ•°é‡
        for cat_key, cat_config in categories_config.items():
            cat_name = cat_config.get('name', cat_key)
            max_count = cat_config.get('max_count', 5)
            min_count = self._get_effective_min_count(cat_name, cat_config.get('min_count', 1))
            
            if cat_name in all_items:
                if len(all_items[cat_name]) > max_count:
                    all_items[cat_name] = all_items[cat_name][:max_count]
                
                if len(all_items[cat_name]) < min_count:
                    logger.warning(f"åˆ†ç±» {cat_name} å†…å®¹ä¸è¶³: {len(all_items[cat_name])}/{min_count}")
                
                logger.info(f"åˆ†ç±» {cat_name} æœ€ç»ˆ: {len(all_items[cat_name])} æ¡")

        self._latest_dedup_urls = run_dedup_urls
        
        return all_items
    
    def _process_training(self, category_config: Dict[str, Any]) -> List[WeeklyItem]:
        """
        å¤„ç†è®­ç»ƒåˆ†ç±»ï¼ˆLeetCode é¢˜ç›®ï¼‰
        
        Args:
            category_config: åˆ†ç±»é…ç½®
            
        Returns:
            WeeklyItem åˆ—è¡¨
        """
        leetcode_config = category_config.get('leetcode', {})
        if not leetcode_config.get('enabled', True):
            return []
        
        count = leetcode_config.get('count', 2)
        difficulties = leetcode_config.get('difficulties', [])
        
        logger.info(f"è·å– LeetCode é¢˜ç›®: {count} é“")
        
        fetcher = LeetCodeFetcher(difficulties)
        problems = fetcher.get_random_problems(count)
        
        items = []
        for problem in problems:
            # ä½¿ç”¨ä¸­æ–‡æ ‡é¢˜
            title = problem.title_cn or problem.title
            
            summary = f"éš¾åº¦ï¼š{problem.difficulty}ã€‚è¿™æ˜¯ä¸€é“ç»å…¸çš„ç®—æ³•é¢˜ç›®ï¼Œå»ºè®®å°è¯•å¤šç§è§£æ³•ï¼Œç†è§£å…¶èƒŒåçš„ç®—æ³•æ€æƒ³ã€‚"
            
            item = WeeklyItem(
                title=title,
                url=problem.url,
                summary=summary,
                is_english=False,
                category="è®­ç»ƒ",
                item_url=problem.url,
                source_url=problem.url
            )
            items.append(item)
        
        logger.info(f"LeetCode é¢˜ç›®å¤„ç†å®Œæˆ: {len(items)} é“")
        return items
    
    def generate(self, dry_run: bool = False) -> Optional[str]:
        """
        ç”Ÿæˆ Weekly
        
        Args:
            dry_run: æ˜¯å¦ä»…æ¨¡æ‹Ÿè¿è¡Œ
            
        Returns:
            ç”Ÿæˆçš„æ–‡ä»¶è·¯å¾„ï¼Œæˆ– None
        """
        issue = self.get_current_issue()
        date = self.get_current_date()
        output_path = self.get_output_path(issue, date)
        
        logger.info("=" * 50)
        logger.info(f"å¼€å§‹ç”Ÿæˆ Weekly NO{issue} ({date})")
        logger.info("=" * 50)
        
        # ç»Ÿä¸€å¤„ç†æ‰€æœ‰æ–‡ç« ï¼ŒAI è‡ªåŠ¨åˆ†ç±»
        categories_data = self._process_all_articles()
        
        # å¤„ç†è®­ç»ƒåˆ†ç±»
        categories_config = self.config.get('categories', {})
        if 'training' in categories_config:
            training_items = self._process_training(categories_config['training'])
            if training_items:
                categories_data['è®­ç»ƒ'] = training_items
        
        if dry_run:
            logger.info("Dry-run æ¨¡å¼ï¼Œè·³è¿‡ä¿å­˜")
            for cat_name, items in categories_data.items():
                logger.info(f"\n{cat_name}:")
                for item in items:
                    logger.info(f"  - {item.title}")
            return None
        
        # æ ¼å¼åŒ–å¹¶ä¿å­˜
        formatter = WeeklyFormatter(output_path)
        saved_path = formatter.save_weekly(issue, date, categories_data)
        
        # æ‰“å°åˆ°æ§åˆ¶å°
        formatter.print_weekly(issue, date, categories_data)

        # æ›´æ–°å»é‡ç¼“å­˜ï¼ˆä»…è®°å½•éè®­ç»ƒé¡¹ï¼‰
        if self.deduplicator:
            dedup_urls = []
            for category_name, items in categories_data.items():
                if category_name == "è®­ç»ƒ":
                    continue
                for item in items:
                    dedup_url = self._build_dedup_key(item.item_url, item.source_url, item.title)
                    if dedup_url:
                        dedup_urls.append(dedup_url)
            if dedup_urls:
                self.deduplicator.mark_batch_processed(dedup_urls)
                logger.info(f"å·²å†™å…¥ Weekly å»é‡ç¼“å­˜: {len(dedup_urls)} æ¡")

        # æ›´æ–°æœŸå·çŠ¶æ€æ–‡ä»¶
        self._set_next_issue(issue)
        
        logger.info("=" * 50)
        logger.info(f"âœ… Weekly NO{issue} ç”Ÿæˆå®Œæˆ")
        logger.info(f"ğŸ“„ æ–‡ä»¶å·²ä¿å­˜åˆ°: {saved_path}")
        logger.info("=" * 50)
        
        return saved_path
