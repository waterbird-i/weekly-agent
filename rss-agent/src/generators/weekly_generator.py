"""
Weekly ç”Ÿæˆå™¨ä¸»æ¨¡å—
è´Ÿè´£åè°ƒå„æ¨¡å—ç”Ÿæˆå‰ç«¯ Weekly
"""

import json
import logging
import re
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any, Optional

import yaml
from openai import OpenAI

from ..core.rss_fetcher import RSSFetcher, Article
from ..core.content_filter import ContentFilter
from ..fetchers.leetcode_fetcher import LeetCodeFetcher, LeetCodeProblem
from ..fetchers.web_fetcher import WebFetcher
from ..formatters.weekly_formatter import WeeklyFormatter, WeeklyItem
from ..utils import truncate_text

logger = logging.getLogger(__name__)


class WeeklyGenerator:
    """å‰ç«¯ Weekly ç”Ÿæˆå™¨"""
    
    def __init__(self, config_path: str = "weekly_config.yaml"):
        """
        åˆå§‹åŒ–ç”Ÿæˆå™¨
        
        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        """
        self.config_path = Path(config_path)
        # é¡¹ç›®æ ¹ç›®å½• (src/generators -> src -> project_root)
        self.project_root = Path(__file__).parent.parent.parent
        if not self.config_path.is_absolute():
            self.config_path = self.project_root / config_path
        
        self.config = self._load_config()
        self._init_ai_client()
    
    def _load_config(self) -> Dict[str, Any]:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            logger.info(f"é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ: {self.config_path}")
            return config
        except Exception as e:
            logger.error(f"åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
            raise
    
    def _save_config(self):
        """ä¿å­˜é…ç½®æ–‡ä»¶ï¼ˆç”¨äºæ›´æ–°æœŸå·ï¼‰"""
        try:
            with open(self.config_path, 'w', encoding='utf-8') as f:
                yaml.dump(self.config, f, allow_unicode=True, default_flow_style=False)
            logger.info(f"é…ç½®æ–‡ä»¶å·²æ›´æ–°: {self.config_path}")
        except Exception as e:
            logger.error(f"ä¿å­˜é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
    
    def _init_ai_client(self):
        """åˆå§‹åŒ– AI å®¢æˆ·ç«¯"""
        ai_config = self.config.get('ai', {})
        self.ai_client = OpenAI(
            api_key=ai_config.get('api_key', ''),
            base_url=ai_config.get('api_base', 'https://200.xstx.info/v1')
        )
        self.ai_model = ai_config.get('model', 'claude-opus-4-5-20251101-thinking')
        self.ai_max_tokens = ai_config.get('max_tokens', 4096)
        self.weekly_prompt = ai_config.get('weekly_prompt', '')
    

    
    def get_current_issue(self) -> int:
        """è·å–å½“å‰æœŸå·"""
        return self.config.get('weekly', {}).get('current_issue', 1)
    
    def get_current_date(self) -> str:
        """è·å–å½“å‰æ—¥æœŸå­—ç¬¦ä¸²"""
        date_format = self.config.get('weekly', {}).get('date_format', '%Y%m%d')
        return datetime.now().strftime(date_format)
    
    def get_output_path(self, issue: int, date: str) -> str:
        """è·å–è¾“å‡ºæ–‡ä»¶è·¯å¾„"""
        template = self.config.get('weekly', {}).get(
            'output_template', 
            'output/NO{issue}.å‰ç«¯Weekly({date}).md'
        )
        path = template.format(issue=issue, date=date)
        if not Path(path).is_absolute():
            path = str(self.project_root / path)
        return path
    
    def _fetch_category_articles(
        self, 
        category_config: Dict[str, Any]
    ) -> List[Article]:
        """
        è·å–æŸä¸ªåˆ†ç±»çš„æ–‡ç« 
        
        Args:
            category_config: åˆ†ç±»é…ç½®
            
        Returns:
            æ–‡ç« åˆ—è¡¨
        """
        feeds = category_config.get('feeds', [])
        if not feeds:
            return []
        
        # åˆ†ç¦» RSS å’Œ æ™®é€šç½‘é¡µ
        rss_feeds = []
        web_urls = []
        
        for feed in feeds:
            url = feed.get('url', '')
            # ç®€å•åˆ¤æ–­æ˜¯å¦ä¸º RSSï¼Œå¾®ä¿¡å…¬ä¼—å·æ–‡ç« é“¾æ¥è‚¯å®šä¸æ˜¯ RSS
            if 'mp.weixin.qq.com' in url or not (url.endswith('.xml') or url.endswith('.rss') or url.endswith('.atom') or 'rss' in url.lower() or 'feed' in url.lower()):
                web_urls.append(feed)
            else:
                rss_feeds.append(feed)
        
        articles = []
        
        # 1. æŠ“å– RSS
        if rss_feeds:
            fetcher = RSSFetcher(rss_feeds)
            articles.extend(fetcher.fetch_all())
        
        # 2. æŠ“å–æ™®é€šç½‘é¡µ
        if web_urls:
            web_fetcher = WebFetcher()
            articles.extend(web_fetcher.fetch_all(web_urls))
        
        if not articles:
            return []
        
        # æ—¶é—´å’Œé•¿åº¦è¿‡æ»¤é…ç½®
        time_hours = self.config.get('time_filter', {}).get('hours', 168)
        pre_filter_config = self.config.get('pre_filter', {})
        min_length = pre_filter_config.get('min_content_length', 50)
        
        filter_config = {
            'time_filter': {'hours': time_hours},
            'pre_filter': {
                'include_keywords': category_config.get('keywords', []),
                'exclude_keywords': [],
                'min_content_length': min_length
            }
        }
        
        content_filter = ContentFilter(filter_config)
        filtered = content_filter.apply_all_filters(articles)
        
        return filtered
    
    def _extract_items(self, article: Article) -> List[Dict[str, Any]]:
        """
        ä½¿ç”¨ AI ä»æ–‡ç« ä¸­æå–å¤šä¸ªæ¡ç›®
        
        Args:
            article: æ–‡ç« å¯¹è±¡
            
        Returns:
            åŒ…å«å¤šä¸ªæ¡ç›®çš„åˆ—è¡¨ï¼Œæ¯ä¸ªæ¡ç›®æœ‰ title, summary, category, is_english
        """
        try:
            content = article.content or article.summary
            content = truncate_text(content, 8000)  # å¢åŠ å†…å®¹é•¿åº¦ä»¥è·å–æ›´å¤šä¿¡æ¯
            
            # è·å–æ‰€æœ‰å¯ç”¨åˆ†ç±»
            categories = self.config.get('categories', {})
            category_names = [cat.get('name', key) for key, cat in categories.items() if key != 'training']
            
            # æ£€æµ‹æ˜¯å¦ä¸ºæ—¥åˆŠ/èšåˆç±»å†…å®¹
            is_daily_digest = any(kw in article.title.lower() or kw in content[:500].lower() 
                                  for kw in ['æ—¥åˆŠ', 'æ—¥æŠ¥', 'ä»Šæ—¥æ‘˜è¦', 'æ¯æ—¥', 'daily', 'å‘¨åˆŠ'])
            
            if is_daily_digest:
                extract_prompt = f"""ä½ æ˜¯ä¸€ä¸ªæŠ€æœ¯èµ„è®¯ç¼–è¾‘åŠ©æ‰‹ã€‚

è¿™æ˜¯ä¸€ç¯‡æ—¥åˆŠ/æ—¥æŠ¥å†…å®¹ï¼ŒåŒ…å«å¤šæ¡ç‹¬ç«‹çš„èµ„è®¯ã€‚è¯·ä»ä¸­æå–æ¯ä¸€æ¡ç‹¬ç«‹çš„æ–°é—»/èµ„è®¯ã€‚

ã€å¯é€‰åˆ†ç±»ã€‘
{', '.join(category_names)}

ã€åˆ†ç±»æŒ‡å—ã€‘
- æ—¶äº‹ï¼šè¡Œä¸šåŠ¨æ€ã€æ”¿ç­–æ–°é—»ã€å…¬å¸èèµ„ã€å¸‚åœºè¶‹åŠ¿ã€äº§ä¸šè§„åˆ’ç­‰ç»¼åˆèµ„è®¯
- AIèµ„è®¯ï¼šAIæ¨¡å‹å‘å¸ƒã€AIäº§å“æ›´æ–°ã€AIæŠ€æœ¯çªç ´ç­‰ä¸AIç›´æ¥ç›¸å…³çš„èµ„è®¯
- æ•™ç¨‹ï¼šæŠ€æœ¯æ•™ç¨‹ã€å·¥ä½œæµåˆ†äº«ã€å­¦ä¹ èµ„æºã€æœ€ä½³å®è·µç­‰
- å·¥å…·ï¼šå¼€æºé¡¹ç›®ã€å¼€å‘å·¥å…·ã€å®ç”¨è½¯ä»¶ç­‰

ã€é‡è¦ã€‘è¿™æ˜¯èšåˆç±»æ—¥åˆŠå†…å®¹ï¼Œä½ å¿…é¡»ï¼š
1. å°†æ—¥åˆŠæ‹†åˆ†æˆç‹¬ç«‹çš„èµ„è®¯æ¡ç›®ï¼Œæ¯æ¡èµ„è®¯å•ç‹¬æå–
2. ä¸è¦æŠŠå¤šæ¡èµ„è®¯åˆå¹¶æˆä¸€ä¸ªæ¡ç›®
3. æ—¥åˆŠä¸­é€šå¸¸æœ‰"äº§å“ä¸åŠŸèƒ½æ›´æ–°"ã€"å‰æ²¿ç ”ç©¶"ã€"è¡Œä¸šå±•æœ›"ç­‰åˆ†ç±»ï¼Œæ¯ä¸ªåˆ†ç±»ä¸‹çš„æ¯ä¸€æ¡éƒ½æ˜¯ç‹¬ç«‹èµ„è®¯
4. æå–æ•°é‡ï¼š5-10æ¡æœ€é‡è¦çš„èµ„è®¯
5. åŠ¡å¿…æ ¹æ®å†…å®¹åˆç†åˆ†é…åˆ°ä¸åŒåˆ†ç±»ï¼Œä¸è¦éƒ½æ”¾åˆ°åŒä¸€åˆ†ç±»

ã€è¾“å‡ºæ ¼å¼ã€‘
ç›´æ¥è¾“å‡º JSON æ•°ç»„ï¼Œä¸è¦ä»»ä½•markdownæ ‡è®°æˆ–å…¶ä»–æ–‡å­—ï¼š
[
  {{"title": "åŒ—äº¬AIäº§ä¸šä¸¤å¹´å†²ä¸‡äº¿", "summary": "åŒ—äº¬å‘å¸ƒä¹å¤§è¡ŒåŠ¨è®¡åˆ’ï¼Œæ ¸å¿ƒäº§ä¸šè§„æ¨¡é¢„è®¡ä»4500äº¿å†²åˆºä¸‡äº¿ã€‚", "category": "æ—¶äº‹", "is_english": false}},
  {{"title": "SeedFoldè¶…è¶ŠAlphaFold3", "summary": "å­—èŠ‚Seedå›¢é˜Ÿå‘å¸ƒåˆ†å­ç»“æ„é¢„æµ‹æ–°æ¨¡å‹ï¼Œè¡¨ç°ä¼˜äºAlphaFold3ã€‚", "category": "AIèµ„è®¯", "is_english": false}},
  {{"title": "å¼€æºç¬”è®°Memosè·4ä¸‡æ˜Ÿ", "summary": "è½»é‡çº§å¼€æºç¬”è®°æœåŠ¡ï¼Œæ”¯æŒè‡ªæ‰˜ç®¡ï¼Œç”¨æˆ·æ•°æ®å®Œå…¨è‡ªä¸»æŒæ§ã€‚", "category": "å·¥å…·", "is_english": false}}
]

å¦‚æœæ— æ³•æå–ï¼Œè¿”å›ç©ºæ•°ç»„ []"""
            else:
                extract_prompt = f"""ä½ æ˜¯ä¸€ä¸ªå‰ç«¯æŠ€æœ¯å‘¨åˆŠç¼–è¾‘åŠ©æ‰‹ã€‚

ä»ä»¥ä¸‹æ–‡ç« å†…å®¹ä¸­æå–æ‰€æœ‰æœ‰ä»·å€¼çš„ç‹¬ç«‹èµ„è®¯æ¡ç›®ã€‚

ã€å¯é€‰åˆ†ç±»ã€‘
{', '.join(category_names)}

ã€æå–è§„åˆ™ã€‘
1. æ¯ä¸ªæ¡ç›®åªæè¿°ä¸€ä»¶å…·ä½“çš„äº‹ï¼Œä¸è¦èšåˆ
2. ä¸ºæ¯ä¸ªæ¡ç›®é€‰æ‹©æœ€åˆé€‚çš„åˆ†ç±»
3. å¦‚æœæ–‡ç« æ˜¯æ—¥åˆŠ/å‘¨åˆŠåˆé›†ï¼Œæå–å…¶ä¸­æ‰€æœ‰é‡è¦çš„ç‹¬ç«‹èµ„è®¯ï¼ˆæœ€å¤š10æ¡ï¼‰
4. å¦‚æœæ–‡ç« åªåŒ…å«å•ä¸€ä¸»é¢˜ï¼Œåªè¿”å›1æ¡
5. è¿‡æ»¤æ‰å¹¿å‘Šã€æ‹›è˜ç­‰æ— å…³å†…å®¹

ã€è¾“å‡ºæ ¼å¼ã€‘
ç›´æ¥è¾“å‡º JSON æ•°ç»„ï¼Œä¸è¦ä»»ä½•markdownæ ‡è®°æˆ–å…¶ä»–æ–‡å­—ï¼š
[{{"title": "15å­—ä»¥å†…çš„ä¸­æ–‡æ ‡é¢˜", "summary": "çº¦100å­—çš„ä¸­æ–‡ç®€ä»‹", "category": "ä»å¯é€‰åˆ†ç±»ä¸­é€‰æ‹©ä¸€ä¸ª", "is_english": trueæˆ–false}}]

å¦‚æœæ²¡æœ‰å¯æå–çš„å†…å®¹ï¼Œè¿”å›ç©ºæ•°ç»„ []"""
            
            user_prompt = f"""æ ‡é¢˜ï¼š{article.title}
æ¥æºï¼š{article.source}
URLï¼š{article.url}

å†…å®¹ï¼š
{content}"""
            
            # æ—¥åˆŠç±»å†…å®¹éœ€è¦æ›´å¤štokenæ¥è¾“å‡ºå¤šä¸ªæ¡ç›®
            max_tokens = 4000 if is_daily_digest else 2000
            logger.info(f"  æ—¥åˆŠæ£€æµ‹: {is_daily_digest}, æ–‡ç« : {article.title[:30]}...")
            
            response = self.ai_client.chat.completions.create(
                model=self.ai_model,
                messages=[
                    {"role": "system", "content": extract_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                max_tokens=max_tokens,
                temperature=0.7
            )
            
            response_text = response.choices[0].message.content
            logger.debug(f"  AIåŸå§‹å“åº”(å‰300å­—): {response_text[:300] if response_text else 'None'}...")
            
            # è§£æ JSON æ•°ç»„
            try:
                # ç§»é™¤å¯èƒ½çš„ markdown ä»£ç å—æ ‡è®°
                clean_text = response_text or ""
                if '```json' in clean_text:
                    clean_text = re.sub(r'```json\s*', '', clean_text)
                    clean_text = re.sub(r'```\s*$', '', clean_text)
                elif '```' in clean_text:
                    clean_text = re.sub(r'```\s*', '', clean_text)
                
                # ç§»é™¤ thinking æ ‡ç­¾ï¼ˆClaudeæ¨¡å‹å¯èƒ½è¿”å›ï¼‰
                clean_text = re.sub(r'<thinking>.*?</thinking>', '', clean_text, flags=re.DOTALL)
                clean_text = re.sub(r'<thinking>.*', '', clean_text, flags=re.DOTALL)
                
                # æå– JSON æ•°ç»„
                json_match = re.search(r'\[.*\]', clean_text, re.DOTALL)
                if json_match:
                    json_str = json_match.group()
                    items = json.loads(json_str)
                    
                    # æ¸…ç†å¹¶è¿”å›æ¡ç›®
                    result = []
                    for idx, item in enumerate(items):
                        if isinstance(item, dict) and item.get('title') and item.get('summary'):
                            item['summary'] = self._clean_summary(item.get('summary', ''))
                            item['source_url'] = article.url
                            result.append(item)
                    
                    # å›¾ç‰‡åˆ†é…ç­–ç•¥ï¼šæ¯æ¡æå–çš„æ–°é—»éƒ½ä¿ç•™åŸæ–‡ç« çš„å›¾ç‰‡
                    for item in result:
                        item['image_url'] = article.image_url
                    
                    if result:
                        logger.info(f"  æˆåŠŸæå– {len(result)} ä¸ªæ¡ç›®")
                        return result
                    else:
                        logger.warning(f"  JSONè§£ææˆåŠŸä½†æ— æœ‰æ•ˆæ¡ç›®")
                else:
                    logger.warning(f"  æœªæ‰¾åˆ°JSONæ•°ç»„")
            except (json.JSONDecodeError, AttributeError) as parse_err:
                logger.warning(f"  JSONè§£æå¤±è´¥: {parse_err}")
            
            # è§£æå¤±è´¥ï¼Œè¿”å›å•æ¡ç›®ï¼ˆå…¼å®¹åŸé€»è¾‘ï¼‰
            # å¯¹äºæ—¥åˆŠç±»å†…å®¹ï¼Œå°è¯•ä»å†…å®¹ä¸­æå–æœ‰æ„ä¹‰çš„æ ‡é¢˜å’Œç®€ä»‹
            fallback_title = self._extract_fallback_title(article)
            fallback_summary = self._extract_fallback_summary(article, fallback_title)
            return [{
                "title": fallback_title,
                "summary": fallback_summary,
                "category": "AIèµ„è®¯" if is_daily_digest else "æ—¶äº‹",
                "is_english": self._detect_english(article.title),
                "source_url": article.url,
                "image_url": article.image_url
            }]
            
        except Exception as e:
            logger.error(f"æå–æ¡ç›®å¤±è´¥: {article.title}, é”™è¯¯: {e}")
            fallback_title = self._extract_fallback_title(article)
            fallback_summary = self._extract_fallback_summary(article, fallback_title)
            return [{
                "title": fallback_title,
                "summary": fallback_summary,
                "category": "AIèµ„è®¯",
                "is_english": self._detect_english(article.title),
                "source_url": article.url,
                "image_url": article.image_url
            }]
    
    def _clean_summary(self, summary: str) -> str:
        """
        æ¸…ç†æ‘˜è¦å†…å®¹ï¼Œç§»é™¤æ— æ•ˆä¿¡æ¯
        
        Args:
            summary: åŸå§‹æ‘˜è¦
            
        Returns:
            æ¸…ç†åçš„æ‘˜è¦
        """
        if not summary:
            return "æš‚æ— æè¿°"
        
        # éœ€è¦è¿‡æ»¤çš„æ— æ•ˆå†…å®¹æ¨¡å¼
        invalid_patterns = [
            r'Article URL:\s*<[^>]+>',
            r'Comments URL:\s*<[^>]+>',
            r'Points:\s*\d+',
            r'# Comments:\s*\d+',
            r'Comments:\s*\d+',
            r'<https?://[^>]+>',  # å°–æ‹¬å·åŒ…è£¹çš„ URL
            r'Article URL:.*',
            r'Comments URL:.*',
            r'<thinking>.*?</thinking>',  # ç§»é™¤ thinking æ ‡ç­¾åŠå…¶å†…å®¹
            r'<thinking>.*',  # ç§»é™¤æœªé—­åˆçš„ thinking æ ‡ç­¾
        ]
        
        clean_text = summary
        for pattern in invalid_patterns:
            clean_text = re.sub(pattern, '', clean_text, flags=re.IGNORECASE | re.MULTILINE | re.DOTALL)
        
        # ç§»é™¤å¤šä½™çš„ç©ºè¡Œå’Œç©ºæ ¼
        clean_text = re.sub(r'\n\s*\n', '\n', clean_text)
        clean_text = clean_text.strip()
        
        # å¦‚æœæ¸…ç†åå†…å®¹å¤ªçŸ­æˆ–ä¸ºç©ºï¼Œè¿”å›é»˜è®¤å€¼
        if not clean_text or len(clean_text) < 10:
            return "æš‚æ— æè¿°"
        
        return clean_text
    
    def _extract_fallback_title(self, article: Article) -> str:
        """
        ä»æ—¥åˆŠç±»æ–‡ç« å†…å®¹ä¸­æå–æœ‰æ„ä¹‰çš„æ ‡é¢˜
        
        å½“AIæå–å¤±è´¥æ—¶ï¼Œå¦‚æœåŸå§‹æ ‡é¢˜æ˜¯æ—¥æœŸæ ¼å¼ï¼ˆå¦‚"2026-01-01æ—¥åˆŠ"ï¼‰ï¼Œ
        å°è¯•ä»å†…å®¹ä¸­æå–ç¬¬ä¸€æ¡æœ‰ä»·å€¼çš„èµ„è®¯æ ‡é¢˜ã€‚
        
        Args:
            article: æ–‡ç« å¯¹è±¡
            
        Returns:
            æå–çš„æ ‡é¢˜ï¼ˆ15å­—ä»¥å†…ï¼‰
        """
        original_title = article.title.strip()
        
        # æ£€æµ‹æ˜¯å¦ä¸ºæ—¥æœŸæ ¼å¼çš„æ—¥åˆŠ/æ—¥æŠ¥æ ‡é¢˜
        date_pattern = r'^\d{4}-?\d{2}-?\d{2}.*?(æ—¥åˆŠ|æ—¥æŠ¥|Daily)'
        is_date_title = re.match(date_pattern, original_title, re.IGNORECASE)
        
        if not is_date_title:
            # ä¸æ˜¯æ—¥æœŸæ ¼å¼ï¼Œç›´æ¥è¿”å›åŸæ ‡é¢˜ï¼ˆæˆªæ–­åˆ°15å­—ï¼‰
            return original_title[:15]
        
        # å°è¯•ä»å†…å®¹ä¸­æå–æœ‰æ„ä¹‰çš„æ ‡é¢˜
        content = article.content or article.summary or ""
        
        # ä¼˜å…ˆä»"ä»Šæ—¥æ‘˜è¦"åæå–ç¬¬ä¸€æ¡æœ‰æ„ä¹‰çš„èµ„è®¯
        # æ ¼å¼é€šå¸¸æ˜¯ï¼šä»Šæ—¥æ‘˜è¦ è±†åŒ…çœ¼é•œ2000å†…å”®è…¾è®¯imaä¸€é”®ç”ŸæˆPPT SeedFoldè¶…...
        summary_match = re.search(r'ä»Šæ—¥æ‘˜è¦\s*([^\n]{5,80})', content)
        if summary_match:
            summary_text = summary_match.group(1).strip()
            # æå–ç¬¬ä¸€ä¸ªæœ‰æ„ä¹‰çš„çŸ­è¯­ï¼ˆé€šå¸¸ä»¥ä¸­æ–‡åè¯/äº§å“å¼€å¤´ï¼‰
            # åŒ¹é…æ¨¡å¼ï¼šäº§å“å+åŠ¨ä½œï¼Œæˆ–è€…å…¬å¸å+äº§å“
            news_patterns = [
                # å…¬å¸/äº§å“å + åŠ¨ä½œï¼ˆå¦‚ï¼šè±†åŒ…çœ¼é•œå¼€å”®ã€è…¾è®¯imaä¸€é”®ç”ŸæˆPPTï¼‰
                r'([A-Za-z\u4e00-\u9fa5]{2,8}(?:çœ¼é•œ|æ¨¡å‹|å¹³å°|å·¥å…·|æ¡†æ¶|ç³»ç»Ÿ)?(?:å¼€å”®|å‘å¸ƒ|ä¸Šçº¿|å¼€æº|æ¨å‡º|è·å¾—|å®Œæˆ|èèµ„|çªç ´|è¶…è¶Š|å†²åˆº)[^\s]*)',
                # äº§å“ç‰ˆæœ¬æ ¼å¼ï¼ˆå¦‚ï¼šSeedFoldè¶…AlphaFold3ï¼‰
                r'([A-Za-z][A-Za-z0-9]{1,10}è¶…[A-Za-z0-9]{2,12})',
                # é€šç”¨çš„"XX+åŠ¨è¯"æ ¼å¼
                r'([\u4e00-\u9fa5A-Za-z]{2,6}(?:AI|çœ¼é•œ|æ¨¡å‹|èŠ¯ç‰‡|å¹³å°)?[\u4e00-\u9fa5]{2,8})',
            ]
            
            for pattern in news_patterns:
                match = re.search(pattern, summary_text)
                if match:
                    extracted = match.group(1).strip()
                    if 4 <= len(extracted) <= 15:
                        return extracted
            
            # å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°ï¼Œå–æ‘˜è¦çš„å‰15å­—
            return summary_text[:15]
        
        # å¤‡ç”¨ï¼šä»å†…å®¹ä¸­åŒ¹é…å¸¸è§çš„èµ„è®¯æ¨¡å¼
        patterns = [
            # ä¸­æ–‡äº§å“/å…¬å¸å + åŠ¨ä½œ
            r'([\u4e00-\u9fa5A-Za-z]{2,8}(?:å…¬æµ‹|å‘å¸ƒ|å¼€æº|ä¸Šçº¿|æ¨å‡º|å¼€å”®|å¼€æ”¾|è·å¾—|å®Œæˆ|å®£å¸ƒ|èèµ„|çªç ´)[^\nã€‚ï¼]{0,8})',
            # å…¬å¸å + äº§å“åŠ¨ä½œ
            r'((?:å°ç±³|å­—èŠ‚|è…¾è®¯|é˜¿é‡Œ|ç™¾åº¦|åä¸º|OpenAI|Meta|Google|å¾®è½¯|Apple|åŒ—äº¬|ä¸Šæµ·)[A-Za-z\u4e00-\u9fa5]{2,12})',
        ]
        
        for pattern in patterns:
            match = re.search(pattern, content[:2000])
            if match:
                extracted = match.group(1).strip()
                extracted = re.sub(r'\s+', '', extracted)
                if 4 <= len(extracted) <= 15:
                    return extracted
        
        # æœ€åå›é€€ï¼šä½¿ç”¨åŸæ ‡é¢˜
        return original_title[:15]
    
    def _extract_fallback_summary(self, article: Article, title: str) -> str:
        """
        ä»æ—¥åˆŠç±»æ–‡ç« å†…å®¹ä¸­æå–ä¸æ ‡é¢˜ç›¸å…³çš„ç®€ä»‹
        
        å½“AIæå–å¤±è´¥æ—¶ï¼Œå°è¯•ä»å†…å®¹ä¸­æ‰¾åˆ°ä¸æ ‡é¢˜ç›¸å…³çš„æè¿°ã€‚
        
        Args:
            article: æ–‡ç« å¯¹è±¡
            title: å·²æå–çš„æ ‡é¢˜
            
        Returns:
            æå–çš„ç®€ä»‹ï¼ˆçº¦100å­—ï¼‰
        """
        content = article.content or article.summary or ""
        
        # å°è¯•ä»å†…å®¹ä¸­æŸ¥æ‰¾ä¸æ ‡é¢˜ç›¸å…³çš„æ®µè½
        # æŸ¥æ‰¾æ ‡é¢˜é™„è¿‘çš„å†…å®¹
        title_keywords = list(filter(lambda x: len(x) >= 2, 
                                     re.findall(r'[A-Za-z]+|\u4e00-\u9fa5{2,}', title)))
        
        if title_keywords:
            # å°è¯•æ‰¾åˆ°åŒ…å«æ ‡é¢˜å…³é”®è¯çš„å¥å­
            for keyword in title_keywords[:3]:
                # æŸ¥æ‰¾å…³é”®è¯æ‰€åœ¨çš„å¥å­
                pattern = rf'[^ã€‚ï¼ï¼Ÿ\n]*{re.escape(keyword)}[^ã€‚ï¼ï¼Ÿ\n]*[ã€‚ï¼ï¼Ÿ]?'
                match = re.search(pattern, content)
                if match:
                    sentence = match.group().strip()
                    # é™åˆ¶é•¿åº¦å¹¶æ¸…ç†
                    if 20 <= len(sentence) <= 150:
                        return self._clean_summary(sentence)
        
        # å¦‚æœæ‰¾ä¸åˆ°ç›¸å…³å†…å®¹ï¼Œæå–"ä»Šæ—¥æ‘˜è¦"åçš„ä¸€æ®µå†…å®¹
        summary_pattern = r'ä»Šæ—¥æ‘˜è¦\s*(.{20,200})'
        summary_match = re.search(summary_pattern, content)
        if summary_match:
            return self._clean_summary(summary_match.group(1)[:150])
        
        # æœ€åå›é€€ï¼šä½¿ç”¨åŸå§‹ç®€ä»‹çš„å‰150å­—
        if article.summary:
            return self._clean_summary(article.summary[:150])
        
        return "æš‚æ— æè¿°"
    
    def _detect_english(self, text: str) -> bool:
        """
        æ£€æµ‹æ–‡æœ¬æ˜¯å¦ä¸ºè‹±æ–‡
        
        Args:
            text: å¾…æ£€æµ‹æ–‡æœ¬
            
        Returns:
            æ˜¯å¦ä¸ºè‹±æ–‡
        """
        if not text:
            return False
        # è®¡ç®—è‹±æ–‡å­—ç¬¦å æ¯”
        english_chars = sum(1 for c in text if c.isalpha() and ord(c) < 128)
        total_chars = sum(1 for c in text if c.isalpha())
        if total_chars == 0:
            return False
        return english_chars / total_chars > 0.7
    
    def _process_all_articles(self) -> Dict[str, List[WeeklyItem]]:
        """
        å¤„ç†æ‰€æœ‰æ–‡ç« ï¼Œç»Ÿä¸€æå–å¹¶æŒ‰åˆ†ç±»å½’ç±»
        
        Returns:
            æŒ‰åˆ†ç±»ååˆ†ç»„çš„ WeeklyItem å­—å…¸
        """
        categories_config = self.config.get('categories', {})
        all_items: Dict[str, List[WeeklyItem]] = {}
        processed_urls = set()
        
        # æ”¶é›†æ‰€æœ‰å”¯ä¸€çš„æ–‡ç« 
        all_articles = []
        for cat_key, cat_config in categories_config.items():
            if cat_key == 'training':
                continue
            articles = self._fetch_category_articles(cat_config)
            for article in articles:
                if article.url not in processed_urls:
                    all_articles.append(article)
                    processed_urls.add(article.url)
        
        logger.info(f"å…±æ”¶é›† {len(all_articles)} ç¯‡å”¯ä¸€æ–‡ç« ")
        
        # å¤„ç†æ¯ç¯‡æ–‡ç« ï¼Œæå–å¤šæ¡ç›®
        for article in all_articles:
            logger.info(f"  å¤„ç†æ–‡ç« : {article.title[:40]}...")
            
            extracted_items = self._extract_items(article)
            logger.info(f"    æå–åˆ° {len(extracted_items)} ä¸ªæ¡ç›®")
            
            for item_data in extracted_items:
                category = item_data.get('category', 'æ—¶äº‹')
                
                # ç¡®ä¿åˆ†ç±»å­˜åœ¨
                if category not in all_items:
                    all_items[category] = []
                
                item = WeeklyItem(
                    title=item_data.get('title', article.title),
                    url=item_data.get('source_url', article.url),
                    summary=item_data.get('summary', 'æš‚æ— æè¿°'),
                    is_english=item_data.get('is_english', False),
                    category=category,
                    short_title=item_data.get('title', ''),
                    image_url=item_data.get('image_url', '')
                )
                all_items[category].append(item)
        
        # æŒ‰é…ç½®çš„ max_count é™åˆ¶æ¯ä¸ªåˆ†ç±»çš„æ•°é‡
        for cat_key, cat_config in categories_config.items():
            cat_name = cat_config.get('name', cat_key)
            max_count = cat_config.get('max_count', 5)
            min_count = cat_config.get('min_count', 1)
            
            if cat_name in all_items:
                if len(all_items[cat_name]) > max_count:
                    all_items[cat_name] = all_items[cat_name][:max_count]
                
                if len(all_items[cat_name]) < min_count:
                    logger.warning(f"åˆ†ç±» {cat_name} å†…å®¹ä¸è¶³: {len(all_items[cat_name])}/{min_count}")
                
                logger.info(f"åˆ†ç±» {cat_name} æœ€ç»ˆ: {len(all_items[cat_name])} æ¡")
        
        return all_items
    
    def _process_training(self, category_config: Dict[str, Any]) -> List[WeeklyItem]:
        """
        å¤„ç†è®­ç»ƒåˆ†ç±»ï¼ˆLeetCode é¢˜ç›®ï¼‰
        
        Args:
            category_config: åˆ†ç±»é…ç½®
            
        Returns:
            WeeklyItem åˆ—è¡¨
        """
        leetcode_config = category_config.get('leetcode', {})
        if not leetcode_config.get('enabled', True):
            return []
        
        count = leetcode_config.get('count', 2)
        difficulties = leetcode_config.get('difficulties', [])
        
        logger.info(f"è·å– LeetCode é¢˜ç›®: {count} é“")
        
        fetcher = LeetCodeFetcher(difficulties)
        problems = fetcher.get_random_problems(count)
        
        items = []
        for problem in problems:
            # ä½¿ç”¨ä¸­æ–‡æ ‡é¢˜
            title = problem.title_cn or problem.title
            
            summary = f"éš¾åº¦ï¼š{problem.difficulty}ã€‚è¿™æ˜¯ä¸€é“ç»å…¸çš„ç®—æ³•é¢˜ç›®ï¼Œå»ºè®®å°è¯•å¤šç§è§£æ³•ï¼Œç†è§£å…¶èƒŒåçš„ç®—æ³•æ€æƒ³ã€‚"
            
            item = WeeklyItem(
                title=title,
                url=problem.url,
                summary=summary,
                is_english=False,
                category="è®­ç»ƒ"
            )
            items.append(item)
        
        logger.info(f"LeetCode é¢˜ç›®å¤„ç†å®Œæˆ: {len(items)} é“")
        return items
    
    def generate(self, dry_run: bool = False) -> Optional[str]:
        """
        ç”Ÿæˆ Weekly
        
        Args:
            dry_run: æ˜¯å¦ä»…æ¨¡æ‹Ÿè¿è¡Œ
            
        Returns:
            ç”Ÿæˆçš„æ–‡ä»¶è·¯å¾„ï¼Œæˆ– None
        """
        issue = self.get_current_issue()
        date = self.get_current_date()
        output_path = self.get_output_path(issue, date)
        
        logger.info("=" * 50)
        logger.info(f"å¼€å§‹ç”Ÿæˆ Weekly NO{issue} ({date})")
        logger.info("=" * 50)
        
        # ç»Ÿä¸€å¤„ç†æ‰€æœ‰æ–‡ç« ï¼ŒAI è‡ªåŠ¨åˆ†ç±»
        categories_data = self._process_all_articles()
        
        # å¤„ç†è®­ç»ƒåˆ†ç±»
        categories_config = self.config.get('categories', {})
        if 'training' in categories_config:
            training_items = self._process_training(categories_config['training'])
            if training_items:
                categories_data['è®­ç»ƒ'] = training_items
        
        if dry_run:
            logger.info("Dry-run æ¨¡å¼ï¼Œè·³è¿‡ä¿å­˜")
            for cat_name, items in categories_data.items():
                logger.info(f"\n{cat_name}:")
                for item in items:
                    logger.info(f"  - {item.title}")
            return None
        
        # æ ¼å¼åŒ–å¹¶ä¿å­˜
        formatter = WeeklyFormatter(output_path)
        saved_path = formatter.save_weekly(issue, date, categories_data)
        
        # æ‰“å°åˆ°æ§åˆ¶å°
        formatter.print_weekly(issue, date, categories_data)
        
        # æ›´æ–°æœŸå·
        self.config['weekly']['current_issue'] = issue + 1
        self._save_config()
        
        logger.info("=" * 50)
        logger.info(f"âœ… Weekly NO{issue} ç”Ÿæˆå®Œæˆ")
        logger.info(f"ğŸ“„ æ–‡ä»¶å·²ä¿å­˜åˆ°: {saved_path}")
        logger.info("=" * 50)
        
        return saved_path
