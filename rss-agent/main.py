#!/usr/bin/env python3
"""
RSS Agent ä¸»å…¥å£
ç”¨äºRSSè®¢é˜…æŠ“å–ã€è¿‡æ»¤å’ŒAIåˆ†æ
"""

import argparse
import logging
import sys
from pathlib import Path

import yaml

from src.core.rss_fetcher import RSSFetcher
from src.core.content_filter import ContentFilter
from src.core.ai_processor import AIProcessor
from src.formatters.output_formatter import OutputFormatter
from src.utils import URLDeduplicator

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def load_config(config_path: str) -> dict:
    """
    åŠ è½½é…ç½®æ–‡ä»¶
    
    Args:
        config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        
    Returns:
        é…ç½®å­—å…¸
    """
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        logger.info(f"é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ: {config_path}")
        return config
    except Exception as e:
        logger.error(f"åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
        sys.exit(1)


def main():
    """ä¸»å‡½æ•°"""
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description='RSS Agent - RSSè®¢é˜…æŠ“å–ä¸AIåˆ†æ')
    parser.add_argument(
        '-c', '--config',
        default='config/config.yaml',
        help='é…ç½®æ–‡ä»¶è·¯å¾„ (é»˜è®¤: config/config.yaml)'
    )
    parser.add_argument(
        '-o', '--output',
        default=None,
        help='è¾“å‡ºæ–‡ä»¶è·¯å¾„ (è¦†ç›–é…ç½®æ–‡ä»¶ä¸­çš„è®¾ç½®)'
    )
    parser.add_argument(
        '-n', '--max-articles',
        type=int,
        default=None,
        help='æœ€å¤§å¤„ç†æ–‡ç« æ•° (è¦†ç›–é…ç½®æ–‡ä»¶ä¸­çš„è®¾ç½®)'
    )
    parser.add_argument(
        '--hours',
        type=int,
        default=None,
        help='æ—¶é—´è¿‡æ»¤èŒƒå›´(å°æ—¶) (è¦†ç›–é…ç½®æ–‡ä»¶ä¸­çš„è®¾ç½®)'
    )
    parser.add_argument(
        '--dry-run',
        action='store_true',
        help='ä»…æŠ“å–å’Œè¿‡æ»¤ï¼Œä¸è°ƒç”¨AIåˆ†æ'
    )
    parser.add_argument(
        '--weekly',
        action='store_true',
        help='ç”Ÿæˆå‰ç«¯ Weekly æŠ¥å‘Š'
    )
    parser.add_argument(
        '--weekly-config',
        default='config/weekly_config.yaml',
        help='Weekly é…ç½®æ–‡ä»¶è·¯å¾„ (é»˜è®¤: config/weekly_config.yaml)'
    )
    parser.add_argument(
        '-v', '--verbose',
        action='store_true',
        help='æ˜¾ç¤ºè¯¦ç»†æ—¥å¿—'
    )
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—çº§åˆ«
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    # Weekly æ¨¡å¼
    if args.weekly:
        from src.generators.weekly_generator import WeeklyGenerator
        
        weekly_config_path = args.weekly_config
        if not Path(weekly_config_path).is_absolute():
            weekly_config_path = Path(__file__).parent / weekly_config_path
        
        generator = WeeklyGenerator(str(weekly_config_path))
        generator.generate(dry_run=args.dry_run)
        return
    
    # ç¡®å®šé…ç½®æ–‡ä»¶è·¯å¾„
    config_path = args.config
    if not Path(config_path).is_absolute():
        config_path = Path(__file__).parent / config_path
    
    # åŠ è½½é…ç½®
    config = load_config(str(config_path))
    
    # å‘½ä»¤è¡Œå‚æ•°è¦†ç›–é…ç½®æ–‡ä»¶
    if args.hours:
        config.setdefault('time_filter', {})['hours'] = args.hours
    
    if args.max_articles:
        config.setdefault('output', {})['max_articles'] = args.max_articles
    
    output_path = args.output or config.get('output', {}).get('file_path', 'output/rss_analysis.md')
    if not Path(output_path).is_absolute():
        output_path = Path(__file__).parent / output_path
    
    max_articles = config.get('output', {}).get('max_articles', 20)
    
    # åˆå§‹åŒ–å»é‡å™¨
    cache_file = config.get('dedup', {}).get('cache_file', 'cache/processed_urls.json')
    if not Path(cache_file).is_absolute():
        cache_file = Path(__file__).parent / cache_file
    
    cache_expire_hours = config.get('dedup', {}).get('cache_expire_hours', 168)
    deduplicator = URLDeduplicator(str(cache_file), cache_expire_hours)
    
    logger.info("=" * 50)
    logger.info("RSS Agent å¯åŠ¨")
    logger.info("=" * 50)
    
    # 1. æŠ“å–RSS
    logger.info("\nğŸ“¡ Step 1: æŠ“å–RSSè®¢é˜…...")
    feeds = config.get('rss_feeds', [])
    if not feeds:
        logger.error("é…ç½®æ–‡ä»¶ä¸­æ²¡æœ‰RSSè®¢é˜…æº")
        sys.exit(1)
    
    fetcher = RSSFetcher(feeds)
    articles = fetcher.fetch_all()
    
    if not articles:
        logger.warning("æœªè·å–åˆ°ä»»ä½•æ–‡ç« ")
        sys.exit(0)
    
    # 2. å†…å®¹è¿‡æ»¤
    logger.info("\nğŸ” Step 2: åº”ç”¨å†…å®¹è¿‡æ»¤...")
    content_filter = ContentFilter(config, deduplicator)
    filtered_articles = content_filter.apply_all_filters(articles)
    
    if not filtered_articles:
        logger.warning("è¿‡æ»¤åæ²¡æœ‰å‰©ä½™æ–‡ç« ")
        sys.exit(0)
    
    logger.info(f"è¿‡æ»¤åå‰©ä½™ {len(filtered_articles)} ç¯‡æ–‡ç« å¾…å¤„ç†")
    
    # 3. AIåˆ†æ (å¦‚æœä¸æ˜¯dry-run)
    if args.dry_run:
        logger.info("\nâ­ï¸ Dry-runæ¨¡å¼ï¼Œè·³è¿‡AIåˆ†æ")
        logger.info("è¿‡æ»¤åçš„æ–‡ç« åˆ—è¡¨:")
        for i, article in enumerate(filtered_articles[:max_articles], 1):
            logger.info(f"  {i}. {article.title}")
            logger.info(f"     URL: {article.url}")
        sys.exit(0)
    
    logger.info("\nğŸ¤– Step 3: è°ƒç”¨AIè¿›è¡Œåˆ†æ...")
    ai_processor = AIProcessor(config)
    results = ai_processor.analyze_batch(filtered_articles, max_articles)
    
    # 4. æ ‡è®°å·²å¤„ç†çš„URL
    logger.info("\nğŸ“ Step 4: æ›´æ–°URLç¼“å­˜...")
    processed_urls = [r.article.url for r in results]
    deduplicator.mark_batch_processed(processed_urls)
    
    # 5. è¾“å‡ºæŠ¥å‘Š
    logger.info("\nğŸ“„ Step 5: ç”ŸæˆMarkdownæŠ¥å‘Š...")
    formatter = OutputFormatter(str(output_path))
    saved_path = formatter.save_report(results)
    
    # æ‰“å°æŠ¥å‘Šåˆ°æ§åˆ¶å°
    formatter.print_report(results)
    
    logger.info("=" * 50)
    logger.info("âœ… RSS Agent æ‰§è¡Œå®Œæˆ")
    logger.info(f"ğŸ“„ æŠ¥å‘Šå·²ä¿å­˜åˆ°: {saved_path}")
    logger.info("=" * 50)


if __name__ == '__main__':
    main()
